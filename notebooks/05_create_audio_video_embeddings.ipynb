{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video and Audio Content Analysis with Amazon Bedrock and Amazon Aurora PostgreSQL pgvector\n",
    "\n",
    "This notebook demonstrates how to process video and audio content using Amazon Bedrock with the [Amazon Titan Multimodal Embeddings G1 model](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) to generate embeddings and store them in an [Amazon Aurora PostgreSQL](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html) database with pgvector for similarity search capabilities.\n",
    "\n",
    "> Create Amazon Aurora PostgreSQL with this [Amazon CDK Stack](../create-audio-video-embeddings/02-aurora-pg-vector/README.md)\n",
    "\n",
    "\n",
    "![Diagram](data/video-embedding.png)\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "This notebook is useful for:\n",
    "- Video and audio content analysis\n",
    "- Semantic search in multimedia content\n",
    "- Implementing multimodal RAG systems\n",
    "- Processing and analyzing transcriptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Architecture\n",
    "\n",
    "The system uses:\n",
    "- **Data Storage**: Amazon S3 for video/audio files and extracted frames\n",
    "- **Database**: Amazon Aurora PostgreSQL with pgvector extension for storing embeddings\n",
    "- **AI Services**: Amazon Bedrock with [Amazon Titan Multimodal Embeddings G1 model](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) for generating embeddings\n",
    "- **Configuration**: AWS Systems Manager Parameter Store for secure configuration\n",
    "\n",
    "![Diagram](data/diagram_video.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Setup\n",
    "\n",
    "### Library Imports\n",
    "The notebook begins by importing the necessary libraries:\n",
    "- `boto3`: [AWS SDK for Python ](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingTheBotoAPI.html)\n",
    "- Standard libraries: `json`, `os`, `base64`, `datetime`, `uuid`, etc.\n",
    "- Custom utilities for transcript processing\n",
    "\n",
    "### AWS Client Configuration\n",
    "AWS service clients are configured:\n",
    "- [Configure AWS credentials](https://docs.aws.amazon.com/braket/latest/developerguide/braket-using-boto3.html)\n",
    "- `s3`: Amazon S3 client for storage operations\n",
    "- `ssm`: [AWS Systems Manager Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html)  for configuration\n",
    "- `bedrock_runtime`: Amazon Bedrock Runtime client for embedding generation\n",
    "- `transcribe_client`: Amazon Transcribe client for audio transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Processing Flow\n",
    "\n",
    "The pipeline processes videos through these steps:\n",
    "\n",
    "1. A video file is processed using Python code in a Jupyter notebook, utilizing the boto3 SDK to interact with AWS services.\n",
    "\n",
    "2. The audio stream is extracted and sent to Amazon Transcribe for speech-to-text conversion.\n",
    "\n",
    "3. Simultaneously, the video is processed to extract key frames, which are stored in an Amazon S3 bucket.\n",
    "\n",
    "4. The extracted frames are processed through Amazon Bedrock's Titan embedding model to generate multimodal vectors that represent the visual content.\n",
    "\n",
    "5. Finally, all the processed data (transcriptions, frame data, and vectors) is stored in Amazon Aurora Serverless PostgreSQL with pgvector extension, enabling vector-based searches through standard RDS API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install boto3\n",
    "#!pip install json\n",
    "#!pip install base64\n",
    "#!pip install uuid\n",
    "# or install requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from PIL import Image as PILImage\n",
    "import random\n",
    "\n",
    "_region_name = \"us-west-2\"\n",
    "ssm = boto3.client(service_name=\"ssm\", region_name=\"us-east-1\")\n",
    "\n",
    "# Default model settings\n",
    "default_model_id = os.environ.get(\"DEFAULT_MODEL_ID\", \"amazon.titan-embed-image-v1\")\n",
    "default_embedding_dimmesion = os.environ.get(\"DEFAULT_EMBEDDING_DIMENSION\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Interface (AuroraPostgres Class)\n",
    "\n",
    "An `AuroraPostgres` class is defined that includes:\n",
    "- `execute_statement()`: Handles database connections and queries [using RDS Data API](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html)\n",
    "- `insert()`: Implements functions to insert embedding data into the database\n",
    "- `similarity_search()`: Provides similarity search using cosine similarity and L2 distance methods\n",
    "\n",
    "Code: [aurora_service.py](create_audio_video_helper/aurora_service.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_audio_video_helper import AuroraPostgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Video Content Processing\n",
    "\n",
    "A `VideoProcessor` class that uses the [ffmpeg libavcodec library](https://ffmpeg.org/) are implemented for:\n",
    "- `extract_sec_number()`: Extracts second numbers from file paths for frame ordering\n",
    "- `ffmpeg_check()`: Checks if ffmpeg is installed on the system\n",
    "- `run_ffmpeg_command()`: Runs ffmpeg commands with proper error handling\n",
    "- `extract_frames()`: Extracts video frames at regular intervals using ffmpeg\n",
    "\n",
    "Code: [video_processor.py](create_audio_video_helper/video_processor.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_audio_video_helper import VideoProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Video Download and Processing\n",
    "\n",
    "A `VideoManager` class is defined that includes:\n",
    "- `parse_location()`: Parses S3 URIs into bucket, prefix, filename components\n",
    "- `read_json_from_s3()`: Reads JSON files from S3 buckets\n",
    "- `download_file()`: Downloads files from S3 to local storage\n",
    "- `upload_file()`: Uploads local files to S3\n",
    "- `read_image_from_s3()`: Reads image files directly from S3\n",
    "- `read_image_from_local()`: Reads image files from local storage\n",
    "\n",
    "Code: [video_manager.py](create_audio_video_helper/video_manager.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_audio_video_helper import VideoManager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Audio Processing with Amazon Transcribe\n",
    "\n",
    "Implements functions for:\n",
    "- `transcribe()`: Starts transcription jobs with Amazon Transcribe\n",
    "- `wait_transcription_complete()`: Waits for job completion with status polling\n",
    "- `get_transcribe_result_data()`: Gets transcription job status and results\n",
    "- `process_part()`: Processes individual parts of the transcript\n",
    "- `process_segments()`: Processes transcript segments with timing information\n",
    "- `combine_by_seconds()`: Combines transcript segments by time\n",
    "- `combine_transcrip_segments_by_speaker()`: Combines segments by speaker for better readability\n",
    "- `process_transcript()`: Main function to process the complete transcript\n",
    "\n",
    "Code: [audio_processor.py](create_audio_video_helper/audio_processing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_audio_video_helper import AudioProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embedding Generation\n",
    "\n",
    "Includes functions for:\n",
    "- `get_image_embeddings()`: Generates embeddings for images using Amazon Bedrock\n",
    "- `get_images_embeddings()`: Processes multiple images with progress bars\n",
    "- `get_embeddings()`: Generic function that handles both text and image embedding generation\n",
    "- `get_text_embeddings()`: Generates embeddings for text using Amazon Bedrock\n",
    "- `create_text_embeddings()`: Creates structured embedding records for transcribed text\n",
    "- `create_frames_embeddings()`: Creates structured embedding records for video frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_audio_video_helper import EmbeddingGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Select Key Frames\n",
    "\n",
    "We select frames where the similarity drops below a threshold (indicating a visual change)\n",
    "\n",
    "Code: [compare_frames.py](create_audio_video_helper/compare_frames.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_audio_video_helper import CompareFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Flow\n",
    "\n",
    "The complete workflow includes:\n",
    "1. Downloading the video file from S3 using `download_file()`\n",
    "2. Verifying ffmpeg installation with `ffmpeg_check()`\n",
    "3. Starting a transcription job with `transcribe()`\n",
    "4. Extracting frames from the video using `extract_frames()`\n",
    "5. Generating embeddings for the extracted frames with `get_images_embeddings()`\n",
    "6. Filtering relevant frames based on similarity using `filter_relevant_frames()`\n",
    "7. Processing the transcription results with `process_transcript()`\n",
    "8. Creating embeddings for the transcribed text and selected frames using `create_text_embeddings()` and `create_frames_embeddings()`\n",
    "9. Inserting the embeddings into the Aurora PostgreSQL database with `aurora.insert()`\n",
    "\n",
    "\n",
    "### Configuration\n",
    "The system uses environment variables and AWS Systems Manager Parameter Store for configuration:\n",
    "\n",
    "**DEFAULT_MODEL_ID:** Bedrock model ID (default: \"amazon.titan-embed-image-v1\")\n",
    "\n",
    "**DEFAULT_EMBEDDING_DIMENSION:** Embedding dimension (default: \"1024\")\n",
    "\n",
    "### SSM Parameters:\n",
    "\n",
    "```\n",
    "/pgvector/cluster_arn\n",
    "```\n",
    "\n",
    "```\n",
    "/pgvector/secret_arn\n",
    "```\n",
    "```\n",
    "/pgvector/table_name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ssm_parameter(name):\n",
    "    response = ssm.get_parameter(Name=name, WithDecryption=True)\n",
    "    return response[\"Parameter\"][\"Value\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data from environment variables, never share secrets!\n",
    "\n",
    "cluster_arn = get_ssm_parameter(\"/videopgvector/cluster_arn\")\n",
    "credentials_arn = get_ssm_parameter(\"/videopgvector/secret_arn\")\n",
    "table_name = get_ssm_parameter(\"/videopgvector/video_table_name\")\n",
    "default_database_name = \"kbdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Aurora PostgreSQL client\n",
    "aurora = AuroraPostgres(cluster_arn, default_database_name, credentials_arn,_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numberOfRecordsUpdated': 0, 'formattedRecords': '[{\"count\":125}]'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify Aurora Cluster conectivity:\n",
    "aurora.execute_statement(\"select count(*) from bedrock_integration.knowledge_bases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Video to Amazon S3 bucket and Obtain s3_uri\n",
    "\n",
    "This code shows how to upload a video from the `tmp` folder to an S3 bucket and obtain the S3 URI needed for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def upload_video_to_s3(video_path, bucket_name, s3_key=None):\n",
    "    \"\"\"\n",
    "    Upload a video file to an S3 bucket and return the S3 URI.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    video_path : str\n",
    "        Local path to the video file\n",
    "    bucket_name : str\n",
    "        Name of the S3 bucket\n",
    "    s3_key : str, optional\n",
    "        The S3 key (path) where the video will be stored. If not provided,\n",
    "        the filename from video_path will be used.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The S3 URI of the uploaded video (s3://bucket-name/key)\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "    \n",
    "    # If s3_key is not provided, use the filename from video_path\n",
    "    if s3_key is None:\n",
    "        s3_key = Path(video_path).name\n",
    "    \n",
    "    # Initialize S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        # Upload the file\n",
    "        print(f\"Uploading {video_path} to s3://{bucket_name}/{s3_key}...\")\n",
    "        s3_client.upload_file(video_path, bucket_name, s3_key)\n",
    "        print(\"Upload successful!\")\n",
    "        \n",
    "        # Construct and return the S3 URI\n",
    "        s3_uri = f\"s3://{bucket_name}/{s3_key}\"\n",
    "        return s3_uri\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Video file not found: tmp/video.mp4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m s3_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos/sample_video.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Subir el video y obtener el S3 URI\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m s3_uri \u001b[38;5;241m=\u001b[39m \u001b[43mupload_video_to_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms3_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS3 URI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms3_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36mupload_video_to_s3\u001b[0;34m(video_path, bucket_name, s3_key)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Check if the file exists\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(video_path):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# If s3_key is not provided, use the filename from video_path\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s3_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Video file not found: tmp/video.mp4"
     ]
    }
   ],
   "source": [
    "# Configure the parameters\n",
    "video_path = \"tmp/video.mp4\"  # Path to the video in the tmp folder\n",
    "bucket_name = \"you-bucket-1234\"     # Name of your S3 bucket\n",
    "\n",
    "\n",
    "# You can also specify a custom path in S3 (optional)\n",
    "s3_key = \"videos/sample_video.mp4\"\n",
    "\n",
    "# Subir el video y obtener el S3 URI\n",
    "s3_uri = upload_video_to_s3(video_path, bucket_name,s3_key)\n",
    "print(f\"S3 URI: {s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "# Create directory if it doesn't exist\n",
    "\n",
    "tmp_path                    = \"./tmp\"\n",
    "\n",
    "#s3_uri = \"s3://you-bucket-1234/videos/you-video.mp4\"\n",
    "\n",
    "s3_uri = \"s3://embeddings-demo-1234/videos/DEV315.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dowloading s3://embeddings-demo-1234/videos/DEV315.mp4 to ./tmp/DEV315.mp4\n",
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "videomanager = VideoManager(s3_uri,_region_name)\n",
    "\n",
    "bucket, prefix, fileName, extension, file  = videomanager.parse_location(s3_uri)\n",
    "\n",
    "local_path              = f\"{tmp_path}/{file}\"\n",
    "location                = f\"{prefix}/{file}\"\n",
    "output_dir              = f\"{tmp_path}/{fileName}\"\n",
    "\n",
    "\n",
    "os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "print(f\"dowloading s3://{bucket}/{prefix}/{file} to {local_path}\")\n",
    "result = videomanager.download_file(bucket,location, local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg: ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "built with Apple clang version 16.0.0 (clang-1600.0.26.6)\n",
      "configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "libavutil      59. 39.100 / 59. 39.100\n",
      "libavcodec     61. 19.101 / 61. 19.101\n",
      "libavformat    61.  7.100 / 61.  7.100\n",
      "libavdevice    61.  3.100 / 61.  3.100\n",
      "libavfilter    10.  4.100 / 10.  4.100\n",
      "libswscale      8.  3.100 /  8.  3.100\n",
      "libswresample   5.  3.100 /  5.  3.100\n",
      "libpostproc    58.  3.100 / 58.  3.100\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify ffmpeg is installed \n",
    "videoprocessor = VideoProcessor()\n",
    "videoprocessor.ffmpeg_check() ## Check if ffmpeg is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the media file\n",
    "This part involves:\n",
    "1. Transcribing the audio to text using Amazon Transcribe\n",
    "2. Extract frames using ffmpeg\n",
    "\n",
    "This notebook assumes you have a valid media file in s3://path/to/video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✅ Start Amazon Transcribe Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_processing = AudioProcessing(_region_name,videomanager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription job DEV315.mp4-250409-115704 started...\n"
     ]
    }
   ],
   "source": [
    "job_name = audio_processing.transcribe(s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✅  Extract Key Frames with ffmpeg and Amazon Bedrock with Titan Multimodal Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing frames...\n",
      "code: 0 stdout:  stderr: frame=  253 fps=0.0 q=7.1 size=N/A time=00:04:13.00 bitrate=N/A speed= 502x    \n",
      "frame=  512 fps=510 q=8.9 size=N/A time=00:08:32.00 bitrate=N/A speed= 510x    \n",
      "frame=  749 fps=497 q=4.7 size=N/A time=00:12:29.00 bitrate=N/A speed= 497x    \n",
      "frame=  951 fps=473 q=8.5 size=N/A time=00:15:51.00 bitrate=N/A speed= 473x    \n",
      "frame= 1215 fps=483 q=3.8 size=N/A time=00:20:15.00 bitrate=N/A speed= 483x    \n",
      "frame= 1490 fps=494 q=10.4 size=N/A time=00:24:50.00 bitrate=N/A speed= 494x    \n",
      "frame= 1734 fps=492 q=7.6 size=N/A time=00:28:54.00 bitrate=N/A speed= 492x    \n",
      "frame= 2015 fps=501 q=9.0 size=N/A time=00:33:35.00 bitrate=N/A speed= 501x    \n",
      "frame= 2097 fps=499 q=7.2 Lsize=N/A time=00:34:57.00 bitrate=N/A speed= 499x    \n",
      "\n",
      "done processing frames => 2097\n"
     ]
    }
   ],
   "source": [
    "files = videoprocessor.extract_frames(local_path, output_dir, every=1) # 1 frame per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Text and Image embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_generation = EmbeddingGeneration(videomanager,_region_name,default_model_id,default_embedding_dimmesion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings:  39%|███▉      | 823/2097 [04:20<06:26,  3.29img/s, file=sec_00823.jpg]"
     ]
    }
   ],
   "source": [
    "# calculate embeddings for all extracted frames (1 per second)\n",
    "embed_1024 = embedding_generation.get_images_embeddings(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareframes = CompareFrames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only different frames by calculating cosine similarity sequentially\n",
    "selected_frames = compareframes.filter_relevant_frames(embed_1024, difference_threshold=0.8) # frame is skipped if is similar to previous \n",
    "\n",
    "print (f\"from {len(embed_1024)} frames to {len(selected_frames)} relevant frames:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✅  Check the transcription Job and process text results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_name = \"XXXX\" # For existing jobs put the job name here\n",
    "transcript_url =audio_processing.wait_transcription_complete(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts, duration = audio_processing.process_transcript(transcript_url, max_chars_per_segment=1000)\n",
    "print (f\"Duration:{duration}s\")\n",
    "for seg, speaker, text in transcripts[:2]:\n",
    "    print (f\"sec: {seg}\\n{speaker}:\\n   {text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_frames_files = [(sf, files[sf])for sf in selected_frames]\n",
    "selected_frames_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = embedding_generation.create_text_embeddings(transcripts, transcript_url)\n",
    "frames_embeddings = embedding_generation.create_frames_embeddings(selected_frames_files, s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Text Embeddings:\\n\")\n",
    "for te in text_embeddings:\n",
    "    print(f\"Chunk:{te.get('chunks')[:50]}, embedding(3): {te.get('embedding')[:3]}, metadata: {te.get('metadata')} \")\n",
    "\n",
    "\n",
    "print (\"\\nImage Embeddings:\\n\")\n",
    "for fe in frames_embeddings:\n",
    "    print(f\"Source:{fe.get('source')}, embedding(3): {fe.get('embedding')[:3]}, metadata: {fe.get('metadata')} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert to Vector Database Aurora PostgreSQL (pgvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aurora.execute_statement(\"select count(*) from bedrock_integration.knowledge_bases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally clean the table\n",
    "#aurora.execute_statement(\"delete from bedrock_integration.knowledge_bases\")\n",
    "aurora.execute_statement(\"select count(*) from bedrock_integration.knowledge_bases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert text embeddings into Aurora PostgreSQL\n",
    "if text_embeddings:\n",
    "    aurora.insert(text_embeddings)\n",
    "    print(f\"Inserted {len(text_embeddings)} text embeddings\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert frame embeddings into Aurora PostgreSQL\n",
    "if frames_embeddings:\n",
    "    aurora.insert(frames_embeddings)\n",
    "    print(f\"Inserted {len(frames_embeddings)} frame embeddings\")\n",
    "\n",
    "\n",
    "aurora.execute_statement(\"select count(*) from bedrock_integration.knowledge_bases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search\n",
    "\n",
    "Implements functions for:\n",
    "- `retrieve()`: Performs similarity searches in the database and displays results\n",
    "- `aurora.similarity_search()`: Executes the vector similarity search in the database\n",
    "- `get_embeddings()`: Generates embeddings for the search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def retrieve(search_query, how=\"cosine\", k=5):\n",
    "    search_vector = embedding_generation.get_embeddings(search_query)\n",
    "    \n",
    "    result = aurora.similarity_search(search_vector,how=how, k=k)\n",
    "    rows = json.loads(result.get(\"formattedRecords\"))\n",
    "    for row in rows:\n",
    "        metric = \"similarity\" if how == \"cosine\" else \"distance\"\n",
    "        metric_value = row.get(metric)\n",
    "        if row.get(\"content_type\") == \"text\":\n",
    "            print (f\"text:\\n{row.get('chunks')}\\n{metric}:{metric_value}\\nmetadata:{row.get('metadata')}\\n\")\n",
    "        if row.get(\"content_type\") == \"image\":\n",
    "            img = PILImage.open(row.get('source'))            \n",
    "            print (f\"Image:\\n{row.get('source')}\\n{metric}:{metric_value}\\nmetadata:{row.get('metadata')}\\n\")\n",
    "            display(img)\n",
    "        del row[\"embedding\"]\n",
    "        del row[\"id\"]\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"elizabeth fuentes guillermo ruiz\"\n",
    "docs = retrieve(search_query, how=\"cosine\", k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search using images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_image = random.choice(files)\n",
    "print(one_image)\n",
    "display(PILImage.open(one_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retrieve(videomanager.read_image_from_local(one_image), how=\"cosine\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Implementation\n",
    "\n",
    "Finally, the notebook implements a complete RAG system:\n",
    "- `CustomMultimodalRetriever`: A custom retriever class that extends BaseRetriever\n",
    "- `_get_relevant_documents()`: Core retrieval method that finds similar content\n",
    "- `image_content_block()`: Formats image content for LLM consumption\n",
    "- `text_content_block()`: Formats text content for LLM consumption\n",
    "- `parse_docs_for_context()`: Processes documents for context (text and images)\n",
    "- `ThinkingLLM`: Uses an LLM to answer questions based on retrieved content\n",
    "\n",
    "> Based on https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_retriever.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "class CustomMultimodalRetriever(BaseRetriever):\n",
    "    \"\"\"A retriever that contains the top k documents that contain the user query.\n",
    "    query could be text or image_bytes\n",
    "    \"\"\"\n",
    "    k: int\n",
    "    \"\"\"Number of top results to return\"\"\"\n",
    "    how: str\n",
    "    \"\"\"How to calculate the similarity between the query and the documents.\"\"\"\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "        search_vector = get_embeddings(query)\n",
    "        result = aurora.similarity_search(search_vector, how=self.how, k=self.k)\n",
    "        rows = json.loads(result.get(\"formattedRecords\"))\n",
    "\n",
    "        matching_documents = []\n",
    "\n",
    "        for row in rows:\n",
    "            document_kwargs = dict(\n",
    "                metadata=dict(**json.loads(row.get(\"metadata\")), content_type = row.get(\"content_type\"), source=row.get(\"sourceurl\")))\n",
    "            \n",
    "            if self.how == \"cosine\":\n",
    "                document_kwargs[\"similarity\"] = row.get(\"similarity\")\n",
    "            elif self.how == \"l2\":\n",
    "                document_kwargs[\"distance\"] = row.get(\"distance\")\n",
    "\n",
    "            if row.get(\"content_type\") == \"text\":\n",
    "                matching_documents.append( Document( page_content=row.get(\"chunks\"), **document_kwargs ))\n",
    "            if row.get(\"content_type\") == \"image\":\n",
    "                matching_documents.append( Document( page_content=row.get(\"source\"),**document_kwargs ))\n",
    "\n",
    "        return matching_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = CustomMultimodalRetriever(how=\"cosine\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"elizabeth?\"\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "budget_tokens = 0\n",
    "max_tokens = 1024\n",
    "conversation: List[Dict] = []\n",
    "reasoning_config = {\"thinking\": {\"type\": \"enabled\", \"budget_tokens\": budget_tokens}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_content_block(image_file):\n",
    "    image_bytes = read_image_from_local(image_file)\n",
    "    extension = image_file.split('.')[-1]\n",
    "    print (f\"Including Image :{image_file}\")\n",
    "    if extension == 'jpg':\n",
    "        extension = 'jpeg'\n",
    "    \n",
    "    block = { \"image\": { \"format\": extension, \"source\": { \"bytes\": image_bytes}}}\n",
    "    return block\n",
    "\n",
    "def text_content_block(text):\n",
    "    return { \"text\": text }\n",
    "\n",
    "def parse_docs_for_context(docs):\n",
    "    blocks = []\n",
    "    for doc in docs:\n",
    "        if doc.metadata.get('content_type') == \"image\":\n",
    "            blocks.append(image_content_block(doc.page_content))\n",
    "        else:\n",
    "            blocks.append(text_content_block(doc.page_content))\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(model_id,system_prompt,content) -> str:\n",
    "    \"\"\"Get completion from Claude model based on conversation history.\n",
    "\n",
    "    Returns:\n",
    "        str: Model completion text\n",
    "    \"\"\"\n",
    "\n",
    "    # Invoke model\n",
    "\n",
    "    kwargs = dict(\n",
    "        modelId=model_id,\n",
    "        inferenceConfig=dict(maxTokens=max_tokens),\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            }\n",
    "        ],\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "    kwargs[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    response = bedrock_runtime.converse(**kwargs)\n",
    "    \n",
    "    return response.get(\"output\",{}).get(\"message\",{}).get(\"content\", [])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_docs = parse_docs_for_context(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"\"\"Answer the user's questions based on the below context. If the context has an image, indicate that it can be reviewed for further feedback.\n",
    "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\". (IF YOU MAKE SOMETHING UP BY YOUR OWN YOU WILL BE FIRED). For each statement in your response provide a [n] where n is the document number that provides the response. \"\"\"\n",
    "model_id = \"us.amazon.nova-pro-v1:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"que dicen sobre los agentes de bedrock?\"\n",
    "docs = retriever.invoke(query)\n",
    "parsed_docs = parse_docs_for_context(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = answer(model_id,system_prompt,[text_content_block(f\"question:{query}\\n\\nDocs:\\n\"), *parsed_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_response[0].get(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"donde se dice elizabeth?\"\n",
    "docs = retriever.invoke(query)\n",
    "parsed_docs = parse_docs_for_context(docs)\n",
    "llm_response = answer(model_id,system_prompt,[text_content_block(f\"question:{query}\\n\\nDocs:\\n\"), *parsed_docs])\n",
    "print(llm_response[0].get(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
