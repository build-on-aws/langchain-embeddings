{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Agentic video RAG with Strands Agents and Aurora PostgreSQL - With containers and API infrastructure\n",
    "\n",
    "Build a production-ready video content analysis system using scalable AWS cloud infrastructure with [Amazon Bedrock](https://aws.amazon.com/bedrock/), [Amazon Transcribe](https://aws.amazon.com/transcribe/), and [Amazon Aurora PostgreSQL](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html) with pgvector extension, [AWS Lambda](https://aws.amazon.com/pm/lambda/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el) and [Amazon API Gateway](https://aws.amazon.com/api-gateway/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el).\n",
    "\n",
    "![Architecture Diagram](../container-video-embeddings/image/diagram.png)\n",
    "\n",
    "This notebook demonstrates [Strands Agents](https://strandsagents.com/) integration with **production-grade AWS cloud infrastructure**. Unlike notebook 06 which processes videos locally, this solution uses deployed AWS services for scalable, enterprise-ready video processing.\n",
    "\n",
    "## 🏗️ Cloud Architecture Components\n",
    "\n",
    "### **Core Processing Services**\n",
    "- **[Amazon ECS](https://aws.amazon.com/ecs/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el)**: Runs containerized video processing tasks using FFmpeg for frame extraction. Handles compute-intensive operations with auto-scaling capabilities.\n",
    "- **[AWS Step Functions](https://aws.amazon.com/step-functions/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el)**: Orchestrates the video processing workflow, coordinating parallel processing streams for visual and audio content with error handling and retry logic.\n",
    "- **[Amazon Transcribe](https://aws.amazon.com/transcribe/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el)**: Converts speech to text with speaker diarization and timestamp accuracy, creating semantic text chunks while maintaining temporal context.\n",
    "- **[Amazon Bedrock](https://aws.amazon.com/bedrock/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el)**: Generates multimodal embeddings using Titan models for both extracted video frames and transcribed text content.\n",
    "\n",
    "### **Storage and API Layer**\n",
    "- **[Amazon Aurora PostgreSQL](https://aws.amazon.com/rds/aurora/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el)**: Stores vector embeddings using pgvector extension, supporting combined semantic searches across visual and audio content.\n",
    "- **[Amazon API Gateway](https://aws.amazon.com/api-gateway/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el)**: Provides RESTful endpoints for video upload, processing status checks, and semantic search queries with authentication and rate limiting.\n",
    "- **[Amazon S3](https://aws.amazon.com/s3/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el)**: Stores original videos, extracted frames, transcription results, and processing artifacts with lifecycle management.\n",
    "- **[AWS Lambda](https://aws.amazon.com/lambda/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el)**: Handles API requests, triggers workflows, and processes results with serverless scaling.\n",
    "\n",
    "## 🔄 Processing Workflow\n",
    "\n",
    "The cloud architecture processes videos through these automated steps:\n",
    "\n",
    "1. **📤 Video Upload**: Video uploaded to S3 bucket triggers Lambda function\n",
    "2. **⚡ Workflow Orchestration**: Step Functions initiates parallel processing streams\n",
    "3. **🎬 Visual Pipeline**: ECS tasks extract frames using containerized FFmpeg → Bedrock generates image embeddings\n",
    "4. **🎤 Audio Pipeline**: Transcribe converts speech to text → Semantic chunking → Bedrock generates text embeddings\n",
    "5. **📊 Vector Storage**: Lambda functions store all embeddings in Aurora PostgreSQL with pgvector\n",
    "6. **🌐 API Access**: API Gateway provides endpoints for search queries and status monitoring\n",
    "\n",
    "## 🤖 Agent Architecture\n",
    "\n",
    "### 1. **Video Analysis Agent** \n",
    "> ⚠️⚠️⚠️⚠️**Infrastructure Requirement**: Deploy the [container-video-embeddings CDK stacks](https://github.com/build-on-aws/langchain-embeddings/tree/main/container-video-embeddings) before running this notebook. ⚠️⚠️⚠️⚠️\n",
    "\n",
    "- **Purpose**: Processes and searches video content using AWS cloud infrastructure\n",
    "- **Capabilities**: Analyzes visual frames, transcribed audio, technical content at scale\n",
    "- **Tools**: `video_embeddings_aws` for cloud-based multimodal video search\n",
    "- **Use Case**: Enterprise-grade technical content analysis, scalable video processing\n",
    "\n",
    "![Architecture Diagram](data/agent_videoembedding_cloud.png)\n",
    "\n",
    "\n",
    "### 2. **Memory-Enhanced Agent**\n",
    "> **Prerequisites**: ⚠️⚠️⚠️⚠️ Create Amazon Aurora PostgreSQL with this [Amazon CDK Stack](https://github.com/build-on-aws/langchain-embeddings/tree/main/create-aurora-pgvector). Follow steps in [05_create_audio_video_embeddings.ipynb](/05_create_audio_video_embeddings.ipynb) and create an [Amazon S3 verctor bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-vectors-buckets-create.html) that will serve as the backend for your vector memory. ⚠️⚠️⚠️⚠️\n",
    "\n",
    "- **Purpose**: Provides personalized, context-aware video analysis with cloud scalability\n",
    "- **Capabilities**: Remembers user preferences, learns from interactions, provides tailored responses\n",
    "- **Tools**: `video_embeddings_aws` + `s3_vector_memory` for persistent user context\n",
    "- **Use Case**: Personalized learning experiences at scale, adaptive content recommendations\n",
    "\n",
    "![Architecture Diagram](data/agent_videoembedding_cloud_memory.png)\n",
    "\n",
    "For detailed explanations of video processing concepts, model options, and S3 memory tools, refer to [notebook 06](06_video_embeddings_with_strands_enhanced.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded\n",
      "🌍 Region: us-east-1\n",
      "🎬 Video Path: videos/video.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import requests\n",
    "import json\n",
    "\n",
    "AWS_REGION = \"us-east-1\"\n",
    "VIDEO_PATH = \"videos/video.mp4\"\n",
    "\n",
    "sys.path.append('tools')\n",
    "from video_embeddings_container import video_embeddings_aws\n",
    "\n",
    "print(f\"✅ Configuration loaded\")\n",
    "print(f\"🌍 Region: {AWS_REGION}\")\n",
    "print(f\"🎬 Video Path: {VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Cloud Tool Configuration\n",
    "\n",
    "The `video_embeddings_aws` tool provides cloud-native video processing:\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|----------|\n",
    "| `action` | 'process', 'search', 'list' | 'process' |\n",
    "| `video_path` | Path to video (local, uploaded to S3) | Required for process |\n",
    "| `query` | Search query (for search action) | Required for search |\n",
    "| `region` | AWS region | 'us-east-1' |\n",
    "| `k` | Number of search results | 10 |\n",
    "| `max_wait_time` | Maximum wait time for processing (seconds) | 300 |\n",
    "\n",
    "### Cloud vs Local Processing:\n",
    "- **Local (notebook 06)**: Direct processing on your machine\n",
    "- **Cloud (this notebook)**: Scalable processing using AWS services\n",
    "- **Performance**: Cloud scales automatically based on demand\n",
    "- **Cost**: Cloud charges only for actual usage\n",
    "- **Reliability**: Cloud provides high availability and fault tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Infrastructure Validation\n",
    "\n",
    "> ⚠️⚠️⚠️⚠️**Infrastructure Requirement**: Deploy the [container-video-embeddings CDK stacks](https://github.com/build-on-aws/langchain-embeddings/tree/main/container-video-embeddings) before running this notebook. ⚠️⚠️⚠️⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Infrastructure parameters found:\n",
      "🔗 API Endpoint: https://eq9dp403p9.execute-api.us-east-1.amazonaws.com/prod/retrieve\n",
      "🪣 S3 Bucket: workflow-stack-videobucket6ed8e1af-vdonyv1wsanm\n",
      "⚙️ State Machine: wfVideoProcessingWorkflow09A1C915-B52uaM9fDUc6\n",
      "🌐 API Status: 200 - ✅ Connected\n"
     ]
    }
   ],
   "source": [
    "ssm_client = boto3.client('ssm', region_name=AWS_REGION)\n",
    "stepfunctions_client = boto3.client('stepfunctions', region_name=AWS_REGION)\n",
    "\n",
    "try:\n",
    "    api_endpoint = ssm_client.get_parameter(Name=\"/videopgvector/api_retrieve\", WithDecryption=True)[\"Parameter\"][\"Value\"]\n",
    "    bucket_name = ssm_client.get_parameter(Name=\"/videopgvector/bucket_name\", WithDecryption=True)[\"Parameter\"][\"Value\"]\n",
    "    state_machine_arn = ssm_client.get_parameter(Name=\"/videopgvector/state_machine_arn\", WithDecryption=True)[\"Parameter\"][\"Value\"]\n",
    "    \n",
    "    print(\"✅ Infrastructure parameters found:\")\n",
    "    print(f\"🔗 API Endpoint: {api_endpoint}\")\n",
    "    print(f\"🪣 S3 Bucket: {bucket_name}\")\n",
    "    print(f\"⚙️ State Machine: {state_machine_arn.split(':')[-1]}\")\n",
    "    \n",
    "    test_payload = {\"query\": \"test\", \"method\": \"retrieve\", \"k\": 1}\n",
    "    response = requests.post(api_endpoint, json=test_payload, timeout=10)\n",
    "    print(f\"🌐 API Status: {response.status_code} - {'✅ Connected' if response.status_code == 200 else '❌ Error'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Infrastructure check failed: {str(e)}\")\n",
    "    print(\"💡 Deploy the container-video-embeddings stacks first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎬 Cloud Video Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎥 About the Video Content\n",
    "\n",
    "This notebook uses the same video from notebooks 05 and 06: **AWS re:Invent 2024 session on \"AI self-service support with knowledge retrieval using PostgreSQL\"** ([YouTube link](https://www.youtube.com/watch?v=fpi3awGakyg)).\n",
    "\n",
    "The video covers:\n",
    "- **Vector databases and embeddings** for AI applications\n",
    "- **Amazon Aurora PostgreSQL with pgvector** for scalable vector storage\n",
    "- **RAG (Retrieval Augmented Generation)** implementations\n",
    "- **Amazon Bedrock Agents** for intelligent customer support\n",
    "- **Real-world use cases** and technical demonstrations\n",
    "\n",
    "This content is perfect for testing our video analysis system with technical presentations that include both visual slides and detailed explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"🎬 Processing video: {VIDEO_PATH}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = video_embeddings_aws(\n",
    "    action=\"process\",\n",
    "    video_path=VIDEO_PATH,\n",
    "    region=AWS_REGION,\n",
    "    max_wait_time=300\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Processing Result: {result.get('status')}\")\n",
    "\n",
    "if result.get('status') == 'success':\n",
    "    print(f\"✅ Video S3 URI: {result.get('video_s3_uri')}\")\n",
    "    print(f\"⏱️ Processing Time: {result.get('processing_time_seconds')}s\")\n",
    "    print(f\"🧠 Embeddings Created: {result.get('embeddings_created')}\")\n",
    "elif result.get('status') == 'processing':\n",
    "    print(f\"⏳ {result.get('message')}\")\n",
    "    print(f\"📍 Video URI: {result.get('video_s3_uri')}\")\n",
    "else:\n",
    "    print(f\"❌ Error: {result.get('message')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Understanding the Processing Results\n",
    "\n",
    "The processing results show:\n",
    "\n",
    "- **Video S3 URI**: Where the original video is stored\n",
    "- **Processing Time**: Total time for the cloud workflow\n",
    "- **Embeddings Created**: Number of vector embeddings generated\n",
    "\n",
    "This data structure supports both visual and semantic search across your video content using the cloud infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Monitor Step Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = stepfunctions_client.list_executions(\n",
    "        stateMachineArn=state_machine_arn,\n",
    "        maxResults=5\n",
    "    )\n",
    "    \n",
    "    print(\"📊 Recent Step Functions Executions:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, execution in enumerate(response['executions'][:3], 1):\n",
    "        status = execution['status']\n",
    "        start_time = execution['startDate'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        name = execution['name']\n",
    "        \n",
    "        status_emoji = {\n",
    "            'RUNNING': '🔄',\n",
    "            'SUCCEEDED': '✅',\n",
    "            'FAILED': '❌',\n",
    "            'TIMED_OUT': '⏰',\n",
    "            'ABORTED': '🛑'\n",
    "        }.get(status, '❓')\n",
    "        \n",
    "        print(f\"{i}. {status_emoji} {status} - {name}\")\n",
    "        print(f\"   Started: {start_time}\")\n",
    "        \n",
    "        if 'stopDate' in execution:\n",
    "            stop_time = execution['stopDate'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            duration = (execution['stopDate'] - execution['startDate']).total_seconds()\n",
    "            print(f\"   Completed: {stop_time} (Duration: {duration:.1f}s)\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking Step Functions: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Search Video Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = [\n",
    "    \"what is aurora\",\n",
    "    \"people talking\",\n",
    "    \"technology\",\n",
    "    \"presentation\"\n",
    "]\n",
    "\n",
    "print(\"🔍 Performing semantic search queries...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(search_queries, 1):\n",
    "    print(f\"\\n{i}. Query: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    search_result = video_embeddings_aws(\n",
    "        action=\"search\",\n",
    "        query=query,\n",
    "        region=AWS_REGION,\n",
    "        k=5\n",
    "    )\n",
    "    \n",
    "    if search_result.get('status') == 'success':\n",
    "        results = search_result.get('results', [])\n",
    "        print(f\"✅ Found {len(results)} results\")\n",
    "        \n",
    "        for j, result in enumerate(results[:3], 1):\n",
    "            similarity = result.get('similarity_score', 0)\n",
    "            content_type = result.get('content_type', 'unknown')\n",
    "            source = result.get('source', 'N/A')\n",
    "            preview = result.get('content_preview', '')\n",
    "            \n",
    "            print(f\"\\n   {j}. Similarity: {similarity:.3f} | Type: {content_type}\")\n",
    "            print(f\"      Source: {source}\")\n",
    "            if preview:\n",
    "                print(f\"      Preview: {preview}\")\n",
    "    else:\n",
    "        print(f\"❌ Search failed: {search_result.get('message')}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Search Results Explained\n",
    "\n",
    "The search results demonstrate multimodal embeddings:\n",
    "\n",
    "- **Type**: 'text' (from transcription) or 'image' (from visual frames)\n",
    "- **Similarity Score**: Higher scores (closer to 1.0) indicate better matches\n",
    "- **Source**: File path or timestamp reference\n",
    "- **Content**: Preview of matched text or frame description\n",
    "\n",
    "This lets you find specific moments in videos using natural language queries, whether the content appears visually or is spoken in the audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: List Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"📋 Listing all processed Frames\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "list_result = video_embeddings_aws(\n",
    "    action=\"list\",\n",
    "    region=AWS_REGION,\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 List Result:\")\n",
    "print(f\"Status: {list_result.get('status')}\")\n",
    "\n",
    "if list_result.get('status') == 'success':\n",
    "    videos = list_result.get('videos', [])\n",
    "    print(f\"📹 Total Frames: {list_result.get('total_videos')}\")\n",
    "    print(f\"🧠 Total Embeddings: {list_result.get('total_embeddings')}\")\n",
    "    \n",
    "    for i, video in enumerate(videos[:5], 1):\n",
    "        print(f\"\\n{i}. Frames:\")\n",
    "        print(f\"   📍 Source: {video.get('source_url')}\")\n",
    "        print(f\"   🎭 Type: {video.get('content_type')}\")\n",
    "        print(f\"   🧠 Embeddings: {video.get('embedding_count')}\")\n",
    "else:\n",
    "    print(f\"❌ Error: {list_result.get('message')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Create agents with Strands Agents\n",
    "\n",
    "Create intelligent agents that use the scalable cloud architecture. For detailed model configuration options, see [notebook 06](06_video_embeddings_with_strands_enhanced.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Model Configuration Options\n",
    "\n",
    "Strands supports multiple model configuration approaches:\n",
    "\n",
    "### Option 1: Default Configuration\n",
    "```python\n",
    "from strands import Agent\n",
    "agent = Agent()  # Uses Claude 4 Sonnet with Amazon Bedrock by default\n",
    "```\n",
    "\n",
    "### Option 2: Specify Model ID\n",
    "```python\n",
    "agent = Agent(model=\"anthropic.claude-sonnet-4-20250514-v1:0\")\n",
    "```\n",
    "\n",
    "### Option 3: BedrockModel (Recommended)\n",
    "```python\n",
    "from strands.models import BedrockModel\n",
    "\n",
    "model = BedrockModel(\n",
    "    model_id=\"anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    temperature=0.3,\n",
    "    top_p=0.8\n",
    ")\n",
    "agent = Agent(model=model)\n",
    "```\n",
    "\n",
    "### Option 4: Anthropic Direct\n",
    "```python\n",
    "from strands.models.anthropic import AnthropicModel\n",
    "\n",
    "model = AnthropicModel(\n",
    "    model_id=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1028,\n",
    "    params={\"temperature\": 0.7}\n",
    ")\n",
    "```\n",
    "\n",
    "You can also use other model providers:\n",
    "- [LiteLLM](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/litellm/)\n",
    "- [llama.cpp](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/llamacpp/)\n",
    "- [Llama API](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/llamaapi/)\n",
    "- [Mistral AI](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/mistral/)\n",
    "- [Ollama](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/ollama/)\n",
    "- [OpenAI](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/openai/)\n",
    "- [Cohere](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/openai/)\n",
    "\n",
    "**BedrockModel Benefits:**\n",
    "- Native AWS integration\n",
    "- Guardrails support\n",
    "- Prompt caching capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cloud-native Strands Video Agent created!\n"
     ]
    }
   ],
   "source": [
    "from strands.models import BedrockModel\n",
    "from strands import Agent\n",
    "from video_image_display import display_video_images\n",
    "\n",
    "model = BedrockModel(model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\")\n",
    "\n",
    "CLOUD_SYSTEM_PROMPT = \"\"\"You are a video processing AI assistant.\n",
    "\n",
    "Available actions:\n",
    "- process: Upload and process videos \n",
    "- search: Search video content using semantic similarity\n",
    "- list: List all processed videos\n",
    "\n",
    "Use video_embeddings_aws for all cloud video operations.\n",
    "Use display_video_images to show search results.\n",
    "\"\"\"\n",
    "\n",
    "cloud_video_agent = Agent(\n",
    "    model=model,\n",
    "    tools=[video_embeddings_aws, display_video_images],\n",
    "    system_prompt=CLOUD_SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "print(\"✅ Cloud-native Strands Video Agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Memory-Enhanced Strands Agent\n",
    "\n",
    "Create an agent with S3 memory capabilities for personalized cloud interactions. For detailed S3 memory tool explanation, see [notebook 06](06_video_embeddings_with_strands_enhanced.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memory-enhanced agent created!\n"
     ]
    }
   ],
   "source": [
    "from s3_memory import s3_vector_memory\n",
    "\n",
    "# S3 Vectors Configuration\n",
    "# S3 Vectors Configuration\n",
    "os.environ['VECTOR_BUCKET_NAME'] = 'YOUR-S3-BUCKET'  # Your S3 Vector bucket\n",
    "os.environ['VECTOR_INDEX_NAME'] = 'YOUR-VECTOR-INDEX'        # Your vector index\n",
    "os.environ['AWS_REGION'] = 'us-east-1'                       # AWS region\n",
    "os.environ['EMBEDDING_MODEL'] = 'amazon.titan-embed-text-v2:0' # Bedrock embedding model\n",
    "\n",
    "cloud_memory_agent = Agent(\n",
    "    model=model,\n",
    "    tools=[video_embeddings_aws, display_video_images, s3_vector_memory],\n",
    "    system_prompt=CLOUD_SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "print(\"✅ Memory-enhanced agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Test Agents\n",
    "\n",
    "Test the agents with intelligent queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Cloud Video Agent\n",
      "==================================================\n",
      "I'll search for content about 'aurora database scalability' in the processed videos and return the top 5 results.\n",
      "Tool #1: video_embeddings_aws\n",
      "Let me try the search again with a default AWS region:\n",
      "Tool #2: video_embeddings_aws\n",
      "🔍 Searching for: 'aurora database scalability'\n",
      "✅ Found 5 matching results\n",
      "Great! I found 5 results related to \"aurora database scalability\". Let me also display the images from these search results to provide you with visual context:\n",
      "Tool #3: display_video_images\n",
      "## Search Results for \"Aurora Database Scalability\"\n",
      "\n",
      "I successfully found 5 relevant results from the video content using semantic search. Here's what I discovered:\n",
      "\n",
      "### **Summary of Findings:**\n",
      "\n",
      "The search returned content from two main video sources discussing database scalability, particularly focusing on AWS services and PostgreSQL compatibility. Here's a breakdown of the top 5 results:\n",
      "\n",
      "**1. Image Content (Highest Similarity: 0.513)**\n",
      "- **Source:** langchain_test_user_2/video.mp4\n",
      "- **Location:** 611 seconds into the video\n",
      "- **Type:** Visual frame from the video discussion\n",
      "\n",
      "**2. Text Content (Similarity: 0.507)**\n",
      "- **Source:** dev315.mp4\n",
      "- **Speaker:** spk_0\n",
      "- **Content Preview:** *\"like PostgreSQL. All while leveraging AWS security and scalability, yeah. And now, after this little spoiler, let's get back to our knowledge base. With the database in place, This is going...\"*\n",
      "\n",
      "**3. Text Content (Similarity: 0.507)**\n",
      "- **Source:** langchain_test_user_2/video.mp4\n",
      "- **Speaker:** spk_0\n",
      "- **Location:** 718 seconds (11:58 timestamp)\n",
      "- **Content Preview:** Same content as result #2, indicating this topic is discussed in multiple videos\n",
      "\n",
      "**4. Image Content (Similarity: 0.504)**\n",
      "- **Source:** dev315.mp4/selected_frames/613.jpg\n",
      "- **Type:** Key frame extracted from the video at timestamp 613\n",
      "\n",
      "**5. Image Content (Similarity: 0.502)**\n",
      "- **Source:** dev315.mp4/selected_frames/470.jpg\n",
      "- **Type:** Key frame extracted from the video at timestamp 470\n",
      "\n",
      "### **Key Insights:**\n",
      "\n",
      "1. **PostgreSQL Compatibility**: The content discusses PostgreSQL-like functionality while leveraging AWS's security and scalability features\n",
      "2. **AWS Integration**: Strong emphasis on AWS services for database scalability solutions\n",
      "3. **Knowledge Base Context**: The discussion appears to be part of a larger conversation about building knowledge bases with scalable database backends\n",
      "4. **Multiple Coverage**: The same content appears in different video sources, suggesting this is an important topic covered across multiple sessions\n",
      "\n",
      "The search used **Vector similarity with cosine distance** and **Amazon Titan Embed** model to find semantically related content, successfully identifying relevant discussions about database scalability in the context of AWS services and PostgreSQL-compatible solutions.Cloud Agent Response: {'role': 'assistant', 'content': [{'text': '## Search Results for \"Aurora Database Scalability\"\\n\\nI successfully found 5 relevant results from the video content using semantic search. Here\\'s what I discovered:\\n\\n### **Summary of Findings:**\\n\\nThe search returned content from two main video sources discussing database scalability, particularly focusing on AWS services and PostgreSQL compatibility. Here\\'s a breakdown of the top 5 results:\\n\\n**1. Image Content (Highest Similarity: 0.513)**\\n- **Source:** langchain_test_user_2/video.mp4\\n- **Location:** 611 seconds into the video\\n- **Type:** Visual frame from the video discussion\\n\\n**2. Text Content (Similarity: 0.507)**\\n- **Source:** dev315.mp4\\n- **Speaker:** spk_0\\n- **Content Preview:** *\"like PostgreSQL. All while leveraging AWS security and scalability, yeah. And now, after this little spoiler, let\\'s get back to our knowledge base. With the database in place, This is going...\"*\\n\\n**3. Text Content (Similarity: 0.507)**\\n- **Source:** langchain_test_user_2/video.mp4\\n- **Speaker:** spk_0\\n- **Location:** 718 seconds (11:58 timestamp)\\n- **Content Preview:** Same content as result #2, indicating this topic is discussed in multiple videos\\n\\n**4. Image Content (Similarity: 0.504)**\\n- **Source:** dev315.mp4/selected_frames/613.jpg\\n- **Type:** Key frame extracted from the video at timestamp 613\\n\\n**5. Image Content (Similarity: 0.502)**\\n- **Source:** dev315.mp4/selected_frames/470.jpg\\n- **Type:** Key frame extracted from the video at timestamp 470\\n\\n### **Key Insights:**\\n\\n1. **PostgreSQL Compatibility**: The content discusses PostgreSQL-like functionality while leveraging AWS\\'s security and scalability features\\n2. **AWS Integration**: Strong emphasis on AWS services for database scalability solutions\\n3. **Knowledge Base Context**: The discussion appears to be part of a larger conversation about building knowledge bases with scalable database backends\\n4. **Multiple Coverage**: The same content appears in different video sources, suggesting this is an important topic covered across multiple sessions\\n\\nThe search used **Vector similarity with cosine distance** and **Amazon Titan Embed** model to find semantically related content, successfully identifying relevant discussions about database scalability in the context of AWS services and PostgreSQL-compatible solutions.'}]}\n"
     ]
    }
   ],
   "source": [
    "print(\"🧪 Testing Cloud Video Agent\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response1 = cloud_video_agent(\n",
    "    \"Search for content about 'aurora database scalability' using the cloud infrastructure. Return the top 5 results and explain what you found.\"\n",
    ")\n",
    "print(f\"Cloud Agent Response: {response1.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll list all the processed videos in the cloud storage and provide you with a comprehensive summary of the available content and processing statistics.\n",
      "Tool #4: video_embeddings_aws\n",
      "📋 Listing all processed videos\n",
      "✅ Found 37 processed videos\n",
      "## 📊 Video Processing Summary & Statistics\n",
      "\n",
      "### **🎬 Available Content Overview**\n",
      "\n",
      "I found **37 processed content items** stored in the cloud, consisting of **2 main video sources** with comprehensive text and visual analysis:\n",
      "\n",
      "---\n",
      "\n",
      "### **📹 Main Video Sources**\n",
      "\n",
      "#### **1. dev315.mp4**\n",
      "- **Location:** `s3://workflow-stack-videobucket6ed8e1af-vdonyv1wsanm/video/dev315.mp4`\n",
      "- **Text Embeddings:** 15 segments\n",
      "- **Image Frames:** 35 extracted key frames\n",
      "- **Content Type:** Multi-speaker technical content\n",
      "- **Key Timestamps:** Frames from 34s to 2097s (35+ minutes of content)\n",
      "\n",
      "#### **2. langchain_test_user_2/video.mp4**\n",
      "- **Location:** `s3://workflow-stack-videobucket6ed8e1af-vdonyv1wsanm/videos/langchain_test_user_2/video.mp4`\n",
      "- **Text Embeddings:** 50 segments (most comprehensive)\n",
      "- **Duration:** Content extends to at least 2089 seconds (34+ minutes)\n",
      "- **Content Type:** Multi-speaker technical presentation\n",
      "\n",
      "---\n",
      "\n",
      "### **📈 Processing Statistics**\n",
      "\n",
      "| Metric | Count |\n",
      "|--------|--------|\n",
      "| **Total Processed Items** | 37 |\n",
      "| **Total Embeddings** | 100 |\n",
      "| **Text Content Segments** | 65 (65%) |\n",
      "| **Image Frame Extractions** | 35 (35%) |\n",
      "| **Unique Video Files** | 2 |\n",
      "| **Storage Backend** | Aurora PostgreSQL with pgvector |\n",
      "\n",
      "---\n",
      "\n",
      "### **🎯 Content Analysis**\n",
      "\n",
      "**Text Content Distribution:**\n",
      "- **dev315.mp4:** 15 text segments (23%)\n",
      "- **langchain_test_user_2/video.mp4:** 50 text segments (77%)\n",
      "\n",
      "**Visual Frame Coverage:**\n",
      "- **35 key frames** extracted from dev315.mp4\n",
      "- Timestamps range from 34 seconds to 2097 seconds\n",
      "- Strategic frame selection covering the entire video duration\n",
      "\n",
      "**Speaker Analysis:**\n",
      "- Multiple speakers identified (spk_0, spk_1)\n",
      "- Technical discussions with AWS/database focus\n",
      "- Content includes PostgreSQL, scalability, and AWS security topics\n",
      "\n",
      "---\n",
      "\n",
      "### **🔧 Technical Infrastructure**\n",
      "\n",
      "- **Embedding Model:** Amazon Titan Embed\n",
      "- **Search Method:** Vector similarity using cosine distance\n",
      "- **Database:** Aurora PostgreSQL with pgvector extension\n",
      "- **Storage:** AWS S3 with organized bucket structure\n",
      "- **Processing:** Automated text transcription and key frame extraction\n",
      "\n",
      "---\n",
      "\n",
      "### **💡 Content Quality Insights**\n",
      "\n",
      "The processed videos appear to be **technical educational content** focusing on:\n",
      "- Database technologies (PostgreSQL, Aurora)\n",
      "- AWS cloud services and scalability\n",
      "- Development frameworks and tools\n",
      "- Knowledge base construction\n",
      "\n",
      "The system has successfully processed **over 1 hour of video content** with comprehensive text transcription and visual analysis, making it fully searchable through semantic queries.\n",
      "Cloud Storage Summary: {'role': 'assistant', 'content': [{'text': '## 📊 Video Processing Summary & Statistics\\n\\n### **🎬 Available Content Overview**\\n\\nI found **37 processed content items** stored in the cloud, consisting of **2 main video sources** with comprehensive text and visual analysis:\\n\\n---\\n\\n### **📹 Main Video Sources**\\n\\n#### **1. dev315.mp4**\\n- **Location:** `s3://workflow-stack-videobucket6ed8e1af-vdonyv1wsanm/video/dev315.mp4`\\n- **Text Embeddings:** 15 segments\\n- **Image Frames:** 35 extracted key frames\\n- **Content Type:** Multi-speaker technical content\\n- **Key Timestamps:** Frames from 34s to 2097s (35+ minutes of content)\\n\\n#### **2. langchain_test_user_2/video.mp4**\\n- **Location:** `s3://workflow-stack-videobucket6ed8e1af-vdonyv1wsanm/videos/langchain_test_user_2/video.mp4`\\n- **Text Embeddings:** 50 segments (most comprehensive)\\n- **Duration:** Content extends to at least 2089 seconds (34+ minutes)\\n- **Content Type:** Multi-speaker technical presentation\\n\\n---\\n\\n### **📈 Processing Statistics**\\n\\n| Metric | Count |\\n|--------|--------|\\n| **Total Processed Items** | 37 |\\n| **Total Embeddings** | 100 |\\n| **Text Content Segments** | 65 (65%) |\\n| **Image Frame Extractions** | 35 (35%) |\\n| **Unique Video Files** | 2 |\\n| **Storage Backend** | Aurora PostgreSQL with pgvector |\\n\\n---\\n\\n### **🎯 Content Analysis**\\n\\n**Text Content Distribution:**\\n- **dev315.mp4:** 15 text segments (23%)\\n- **langchain_test_user_2/video.mp4:** 50 text segments (77%)\\n\\n**Visual Frame Coverage:**\\n- **35 key frames** extracted from dev315.mp4\\n- Timestamps range from 34 seconds to 2097 seconds\\n- Strategic frame selection covering the entire video duration\\n\\n**Speaker Analysis:**\\n- Multiple speakers identified (spk_0, spk_1)\\n- Technical discussions with AWS/database focus\\n- Content includes PostgreSQL, scalability, and AWS security topics\\n\\n---\\n\\n### **🔧 Technical Infrastructure**\\n\\n- **Embedding Model:** Amazon Titan Embed\\n- **Search Method:** Vector similarity using cosine distance\\n- **Database:** Aurora PostgreSQL with pgvector extension\\n- **Storage:** AWS S3 with organized bucket structure\\n- **Processing:** Automated text transcription and key frame extraction\\n\\n---\\n\\n### **💡 Content Quality Insights**\\n\\nThe processed videos appear to be **technical educational content** focusing on:\\n- Database technologies (PostgreSQL, Aurora)\\n- AWS cloud services and scalability\\n- Development frameworks and tools\\n- Knowledge base construction\\n\\nThe system has successfully processed **over 1 hour of video content** with comprehensive text transcription and visual analysis, making it fully searchable through semantic queries.'}]}\n"
     ]
    }
   ],
   "source": [
    "response2 = cloud_video_agent(\n",
    "    \"List all processed videos in the cloud storage and provide a summary of available content with processing statistics.\"\n",
    ")\n",
    "print(f\"\\nCloud Storage Summary: {response2.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3. Processing video (if available):\")\n",
    "response3 = cloud_video_agent(\n",
    "    f\"If the video file {VIDEO_PATH} exists, process it. Wait up to 2 minutes and report results.\"\n",
    ")\n",
    "print(f\"Agent Response: {response3.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you with that comprehensive workflow. Let me start by storing your interests, then search for video content, and finally provide a personalized summary.\n",
      "Tool #1: s3_vector_memory\n",
      "\n",
      "Tool #2: video_embeddings_aws\n",
      "Let me try the search with a region specified:\n",
      "Tool #3: video_embeddings_aws\n",
      "🔍 Searching for: 'production deployment'\n",
      "✅ Found 5 matching results\n",
      "Now let me search for scalability content:\n",
      "Tool #4: video_embeddings_aws\n",
      "🔍 Searching for: 'scalability'\n",
      "✅ Found 5 matching results\n",
      "Now let me store the key insights about cloud-native video processing based on the search results:\n",
      "Tool #5: s3_vector_memory\n",
      "Now let me retrieve your stored interests to create a personalized summary:\n",
      "Tool #6: s3_vector_memory\n",
      "## Personalized Summary Based on Your Cloud Architecture Interests\n",
      "\n",
      "Based on your interests in **cloud architecture and scalable video processing**, here's what I discovered from the video content analysis:\n",
      "\n",
      "### 🏗️ **Production Deployment Insights**\n",
      "- **Deployment Efficiency**: AWS stack deployments complete in approximately 18 minutes, demonstrating rapid infrastructure provisioning\n",
      "- **Infrastructure Management**: Comprehensive resource monitoring and management capabilities are built into the deployment process\n",
      "- **Model Selection**: Cloud Haiku model is utilized for testing video processing services, showing practical AI integration\n",
      "\n",
      "### ⚡ **Scalability Patterns**\n",
      "- **AWS Security & Scalability**: The architecture leverages AWS's native security and scalability features, particularly for PostgreSQL databases\n",
      "- **Storage Architecture**: S3 buckets serve as the foundation for scalable video storage, enabling distributed content management\n",
      "- **Resource Optimization**: The system includes inline recommendations and iterative improvements for code optimization\n",
      "\n",
      "### 🎯 **Key Takeaways for Cloud-Native Video Processing**\n",
      "1. **Fast Deployment**: 18-minute deployment cycles enable rapid iteration and testing\n",
      "2. **Managed Services**: Integration with AWS managed services (PostgreSQL, S3) reduces operational overhead\n",
      "3. **AI Integration**: Seamless incorporation of AI models for video processing and analysis\n",
      "4. **Scalable Storage**: S3-based architecture supports growing video content libraries\n",
      "5. **Security by Design**: Built-in AWS security features protect video processing workflows\n",
      "\n",
      "### 💡 **Recommendations for Your Focus Areas**\n",
      "Given your interest in production deployment strategies and scalability patterns, this architecture demonstrates:\n",
      "- **Efficient CI/CD**: Quick deployment cycles suitable for production environments\n",
      "- **Horizontal Scaling**: AWS-native services that scale automatically with demand\n",
      "- **Cost Optimization**: Managed services reduce infrastructure management overhead\n",
      "- **Monitoring & Observability**: Built-in resource tracking for production monitoring\n",
      "\n",
      "This cloud-native approach aligns perfectly with modern scalable video processing requirements, offering both the performance and reliability needed for production deployments.\n",
      "Personalized Cloud Analysis: {'role': 'assistant', 'content': [{'text': \"## Personalized Summary Based on Your Cloud Architecture Interests\\n\\nBased on your interests in **cloud architecture and scalable video processing**, here's what I discovered from the video content analysis:\\n\\n### 🏗️ **Production Deployment Insights**\\n- **Deployment Efficiency**: AWS stack deployments complete in approximately 18 minutes, demonstrating rapid infrastructure provisioning\\n- **Infrastructure Management**: Comprehensive resource monitoring and management capabilities are built into the deployment process\\n- **Model Selection**: Cloud Haiku model is utilized for testing video processing services, showing practical AI integration\\n\\n### ⚡ **Scalability Patterns**\\n- **AWS Security & Scalability**: The architecture leverages AWS's native security and scalability features, particularly for PostgreSQL databases\\n- **Storage Architecture**: S3 buckets serve as the foundation for scalable video storage, enabling distributed content management\\n- **Resource Optimization**: The system includes inline recommendations and iterative improvements for code optimization\\n\\n### 🎯 **Key Takeaways for Cloud-Native Video Processing**\\n1. **Fast Deployment**: 18-minute deployment cycles enable rapid iteration and testing\\n2. **Managed Services**: Integration with AWS managed services (PostgreSQL, S3) reduces operational overhead\\n3. **AI Integration**: Seamless incorporation of AI models for video processing and analysis\\n4. **Scalable Storage**: S3-based architecture supports growing video content libraries\\n5. **Security by Design**: Built-in AWS security features protect video processing workflows\\n\\n### 💡 **Recommendations for Your Focus Areas**\\nGiven your interest in production deployment strategies and scalability patterns, this architecture demonstrates:\\n- **Efficient CI/CD**: Quick deployment cycles suitable for production environments\\n- **Horizontal Scaling**: AWS-native services that scale automatically with demand\\n- **Cost Optimization**: Managed services reduce infrastructure management overhead\\n- **Monitoring & Observability**: Built-in resource tracking for production monitoring\\n\\nThis cloud-native approach aligns perfectly with modern scalable video processing requirements, offering both the performance and reliability needed for production deployments.\"}]}\n"
     ]
    }
   ],
   "source": [
    "USER_ID = \"cloud_test_user\"\n",
    "\n",
    "response3 = cloud_memory_agent(f\"\"\"For user {USER_ID}:\n",
    "1. Store my interest in cloud architecture and scalable video processing\n",
    "2. Search the video content for 'production deployment' and 'scalability'\n",
    "3. Remember key insights about cloud-native video processing\n",
    "4. Provide a personalized summary based on my cloud architecture interests\"\"\")\n",
    "\n",
    "print(f\"\\nPersonalized Cloud Analysis: {response3.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #7: video_embeddings_aws\n",
      "📋 Listing all processed videos\n",
      "✅ Found 37 processed videos\n",
      "Based on the database query, here's the current video content status:\n",
      "\n",
      "## 📊 Video Database Summary\n",
      "\n",
      "**Total Videos in Database: 2 video files**\n",
      "\n",
      "### 📹 Video Files:\n",
      "1. **`dev315.mp4`** - 15 text embeddings\n",
      "2. **`langchain_test_user_2/video.mp4`** - 50 text embeddings\n",
      "\n",
      "### 🖼️ Video Frames:\n",
      "- **35 extracted image frames** from `dev315.mp4` (1 embedding each)\n",
      "\n",
      "### 📈 Database Statistics:\n",
      "- **Total Content Items**: 37 (2 videos + 35 image frames)\n",
      "- **Total Embeddings**: 100 \n",
      "- **Storage Backend**: Aurora PostgreSQL with pgvector\n",
      "- **Embedding Model**: Amazon Titan Embed\n",
      "\n",
      "### 🔍 Content Breakdown:\n",
      "- **Text Content**: 65 embeddings (from video transcripts/audio)\n",
      "- **Image Content**: 35 embeddings (from extracted video frames)\n",
      "- **Searchable Segments**: All content is semantically searchable\n",
      "\n",
      "The system has processed both videos completely, extracting both audio/text content and key visual frames for comprehensive search capabilities. Each video appears to be well-indexed with multiple searchable segments covering different timestamps and speakers.\n",
      "Personalized Cloud Analysis: {'role': 'assistant', 'content': [{'text': \"Based on the database query, here's the current video content status:\\n\\n## 📊 Video Database Summary\\n\\n**Total Videos in Database: 2 video files**\\n\\n### 📹 Video Files:\\n1. **`dev315.mp4`** - 15 text embeddings\\n2. **`langchain_test_user_2/video.mp4`** - 50 text embeddings\\n\\n### 🖼️ Video Frames:\\n- **35 extracted image frames** from `dev315.mp4` (1 embedding each)\\n\\n### 📈 Database Statistics:\\n- **Total Content Items**: 37 (2 videos + 35 image frames)\\n- **Total Embeddings**: 100 \\n- **Storage Backend**: Aurora PostgreSQL with pgvector\\n- **Embedding Model**: Amazon Titan Embed\\n\\n### 🔍 Content Breakdown:\\n- **Text Content**: 65 embeddings (from video transcripts/audio)\\n- **Image Content**: 35 embeddings (from extracted video frames)\\n- **Searchable Segments**: All content is semantically searchable\\n\\nThe system has processed both videos completely, extracting both audio/text content and key visual frames for comprehensive search capabilities. Each video appears to be well-indexed with multiple searchable segments covering different timestamps and speakers.\"}]}\n"
     ]
    }
   ],
   "source": [
    "response4 = cloud_memory_agent(f\"I am {USER_ID}, How many videos are there in the database?\")\n",
    "print(f\"\\nPersonalized Cloud Analysis: {response4.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me search for content from each video to provide you with comprehensive summaries.\n",
      "Tool #8: video_embeddings_aws\n",
      "🔍 Searching for: 'dev315'\n",
      "✅ Found 10 matching results\n",
      "Let me get more comprehensive content by searching for general topics from each video:\n",
      "Tool #9: video_embeddings_aws\n",
      "🔍 Searching for: 'AWS cloud infrastructure deployment database'\n",
      "✅ Found 15 matching results\n",
      "\n",
      "Tool #10: video_embeddings_aws\n",
      "🔍 Searching for: 'knowledge base AI vector database application'\n",
      "✅ Found 10 matching results\n",
      "Now let me store this comprehensive summary in your memory:\n",
      "Tool #11: s3_vector_memory\n",
      "## 📹 Comprehensive Video Content Summary\n",
      "\n",
      "Based on my analysis of both videos in the database, here are the detailed summaries:\n",
      "\n",
      "### Video 1: `dev315.mp4` (15 text embeddings)\n",
      "### Video 2: `langchain_test_user_2/video.mp4` (50 text embeddings)\n",
      "\n",
      "**Note**: These appear to be the same content with different processing granularity.\n",
      "\n",
      "---\n",
      "\n",
      "## 🎯 **Main Topics Covered**\n",
      "\n",
      "### 1. **AWS Infrastructure as Code with CDK**\n",
      "- **CDK with Python**: Using AWS CDK (Cloud Development Kit) with Python as the backbone of modern infrastructure\n",
      "- **Deployment Efficiency**: Automated deployment processes taking approximately 18 minutes\n",
      "- **Error Reduction**: Eliminating repetitive manual tasks and minimizing human errors\n",
      "- **Language Flexibility**: No specific programming language requirements - choose based on your skills\n",
      "\n",
      "### 2. **Vector Database Implementation**\n",
      "- **Core Concept**: Vector databases as the first building block for AI applications\n",
      "- **Storage & Retrieval**: Creating a \"memory\" system where information is stored and easily retrievable\n",
      "- **Database Options**: \n",
      "  - **PostgreSQL with pgvector** (chosen solution)\n",
      "  - **Amazon OpenSearch** (alternative option)\n",
      "- **Use Case Focus**: Both solutions work but are catered for different AI tasks\n",
      "\n",
      "### 3. **AWS Cloud Architecture**\n",
      "- **Security & Scalability**: Leveraging AWS native security and scalability features\n",
      "- **PostgreSQL Integration**: Using managed PostgreSQL services with AWS security\n",
      "- **S3 Integration**: S3 buckets for storage and resource management\n",
      "- **Resource Management**: Comprehensive stack monitoring and resource tracking\n",
      "\n",
      "### 4. **AI/ML Integration**\n",
      "- **Knowledge Base Creation**: Building knowledge bases with vector database foundations\n",
      "- **Model Selection**: Using Cloud Haiku model for testing and development\n",
      "- **Personalized AI Agents**: Building customized AI agents using the infrastructure\n",
      "- **Multi-language Support**: Amazon Transcribe and foundation models supporting different languages\n",
      "- **Cost Efficiency**: Amazon Bedrock for cost-efficient AI task processing\n",
      "\n",
      "### 5. **Development Best Practices**\n",
      "- **Code Improvements**: Inline recommendations and iterative development processes\n",
      "- **File Documentation**: Automated code explanation and highlighting features\n",
      "- **Multiple Iterations**: Refinement processes for optimal implementation\n",
      "- **Testing Environment**: Comprehensive testing setup with model selection options\n",
      "\n",
      "---\n",
      "\n",
      "## 🔧 **Technical Implementation Details**\n",
      "\n",
      "### **Deployment Process**:\n",
      "1. **Infrastructure Setup**: 18-minute automated deployment cycle\n",
      "2. **Stack Creation**: Complete AWS resource provisioning\n",
      "3. **Database Configuration**: PostgreSQL with pgvector setup\n",
      "4. **Security Implementation**: AWS-native security features\n",
      "5. **Testing & Validation**: Model testing with Cloud Haiku\n",
      "\n",
      "### **Architecture Components**:\n",
      "- **Vector Database**: PostgreSQL with pgvector extension\n",
      "- **Storage Layer**: S3 buckets for data management\n",
      "- **AI/ML Layer**: Amazon Bedrock integration\n",
      "- **Security Layer**: AWS IAM and native security features\n",
      "- **Monitoring**: Comprehensive resource tracking and management\n",
      "\n",
      "---\n",
      "\n",
      "## 💡 **Key Value Propositions**\n",
      "\n",
      "1. **Rapid Deployment**: 18-minute infrastructure provisioning\n",
      "2. **Scalable Architecture**: AWS-native scaling capabilities\n",
      "3. **Cost Optimization**: Efficient resource utilization with managed services\n",
      "4. **AI-Ready Infrastructure**: Purpose-built for modern AI applications\n",
      "5. **Developer-Friendly**: Language-agnostic approach with automated recommendations\n",
      "\n",
      "This content appears to be a comprehensive tutorial or presentation on building scalable, cloud-native AI infrastructure using AWS services, with a particular focus on vector databases for knowledge management and AI applications.\n",
      "Personalized Cloud Analysis: {'role': 'assistant', 'content': [{'text': '## 📹 Comprehensive Video Content Summary\\n\\nBased on my analysis of both videos in the database, here are the detailed summaries:\\n\\n### Video 1: `dev315.mp4` (15 text embeddings)\\n### Video 2: `langchain_test_user_2/video.mp4` (50 text embeddings)\\n\\n**Note**: These appear to be the same content with different processing granularity.\\n\\n---\\n\\n## 🎯 **Main Topics Covered**\\n\\n### 1. **AWS Infrastructure as Code with CDK**\\n- **CDK with Python**: Using AWS CDK (Cloud Development Kit) with Python as the backbone of modern infrastructure\\n- **Deployment Efficiency**: Automated deployment processes taking approximately 18 minutes\\n- **Error Reduction**: Eliminating repetitive manual tasks and minimizing human errors\\n- **Language Flexibility**: No specific programming language requirements - choose based on your skills\\n\\n### 2. **Vector Database Implementation**\\n- **Core Concept**: Vector databases as the first building block for AI applications\\n- **Storage & Retrieval**: Creating a \"memory\" system where information is stored and easily retrievable\\n- **Database Options**: \\n  - **PostgreSQL with pgvector** (chosen solution)\\n  - **Amazon OpenSearch** (alternative option)\\n- **Use Case Focus**: Both solutions work but are catered for different AI tasks\\n\\n### 3. **AWS Cloud Architecture**\\n- **Security & Scalability**: Leveraging AWS native security and scalability features\\n- **PostgreSQL Integration**: Using managed PostgreSQL services with AWS security\\n- **S3 Integration**: S3 buckets for storage and resource management\\n- **Resource Management**: Comprehensive stack monitoring and resource tracking\\n\\n### 4. **AI/ML Integration**\\n- **Knowledge Base Creation**: Building knowledge bases with vector database foundations\\n- **Model Selection**: Using Cloud Haiku model for testing and development\\n- **Personalized AI Agents**: Building customized AI agents using the infrastructure\\n- **Multi-language Support**: Amazon Transcribe and foundation models supporting different languages\\n- **Cost Efficiency**: Amazon Bedrock for cost-efficient AI task processing\\n\\n### 5. **Development Best Practices**\\n- **Code Improvements**: Inline recommendations and iterative development processes\\n- **File Documentation**: Automated code explanation and highlighting features\\n- **Multiple Iterations**: Refinement processes for optimal implementation\\n- **Testing Environment**: Comprehensive testing setup with model selection options\\n\\n---\\n\\n## 🔧 **Technical Implementation Details**\\n\\n### **Deployment Process**:\\n1. **Infrastructure Setup**: 18-minute automated deployment cycle\\n2. **Stack Creation**: Complete AWS resource provisioning\\n3. **Database Configuration**: PostgreSQL with pgvector setup\\n4. **Security Implementation**: AWS-native security features\\n5. **Testing & Validation**: Model testing with Cloud Haiku\\n\\n### **Architecture Components**:\\n- **Vector Database**: PostgreSQL with pgvector extension\\n- **Storage Layer**: S3 buckets for data management\\n- **AI/ML Layer**: Amazon Bedrock integration\\n- **Security Layer**: AWS IAM and native security features\\n- **Monitoring**: Comprehensive resource tracking and management\\n\\n---\\n\\n## 💡 **Key Value Propositions**\\n\\n1. **Rapid Deployment**: 18-minute infrastructure provisioning\\n2. **Scalable Architecture**: AWS-native scaling capabilities\\n3. **Cost Optimization**: Efficient resource utilization with managed services\\n4. **AI-Ready Infrastructure**: Purpose-built for modern AI applications\\n5. **Developer-Friendly**: Language-agnostic approach with automated recommendations\\n\\nThis content appears to be a comprehensive tutorial or presentation on building scalable, cloud-native AI infrastructure using AWS services, with a particular focus on vector databases for knowledge management and AI applications.'}]}\n"
     ]
    }
   ],
   "source": [
    "response5 = cloud_memory_agent(f\"I am {USER_ID}, Give me a summary of the content of each video\")\n",
    "print(f\"\\nPersonalized Cloud Analysis: {response5.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Production Deployment Considerations\n",
    "\n",
    "### **Scaling Configuration**\n",
    "- **ECS Auto Scaling**: Configure based on CPU/memory utilization\n",
    "- **Aurora Scaling**: Use Aurora Serverless v2 for automatic scaling\n",
    "- **API Gateway**: Configure throttling and caching for optimal performance\n",
    "\n",
    "### **Cost Optimization**\n",
    "- **S3 Intelligent Tiering**: Automatically move infrequently accessed videos to cheaper storage\n",
    "- **Spot Instances**: Use for ECS tasks to reduce compute costs\n",
    "- **Reserved Capacity**: Purchase reserved capacity for predictable workloads\n",
    "\n",
    "### **Security Best Practices**\n",
    "- **IAM Roles**: Use least privilege access for all services\n",
    "- **VPC Configuration**: Deploy in private subnets with NAT Gateway\n",
    "- **Encryption**: Use encryption at rest and in transit for all data\n",
    "\n",
    "### **Monitoring and Alerting**\n",
    "- **CloudWatch Dashboards**: Monitor processing times, error rates, costs\n",
    "- **X-Ray Tracing**: Track request flows through the system\n",
    "- **SNS Notifications**: Alert on processing failures or cost thresholds\n",
    "\n",
    "## 🔗 Next Steps\n",
    "\n",
    "- Explore the [container-video-embeddings repository](https://github.com/build-on-aws/langchain-embeddings/tree/main/container-video-embeddings) for deployment details\n",
    "- Test with your own video content at scale\n",
    "- Integrate the API endpoints into your applications\n",
    "- Set up monitoring and alerting for production workloads\n",
    "- Configure auto-scaling policies based on your usage patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
