{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook for loading documents from PDFs, extracting and splitting text, generating embeddings from the text using an  [Amazon Titan Embeddings G1 - Text models](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html) and [LangChain](https://python.langchain.com/docs/get_started/introduction), and then storing those embeddings in [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) for later search/retrieval, potentially grouping text at the semantic level rather than the character level.\n",
    "\n",
    "You will learn how to:\n",
    "- Load text from a PDF file.\n",
    "- Generate embeddings for the text.\n",
    "- Split the text into chunks based on a text splitter or the semantic meaning derived from the embeddings.\n",
    "- Return the semantically split documents.\n",
    "- Create a local vector database.\n",
    "\n",
    "![Diagram](build_pdf_vector_db.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements: \n",
    "- Install boto3 - This is the [AWS SDK for Python ](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingTheBotoAPI.html)that allows interacting with AWS services. Install with `pip install boto3`.\n",
    "- [Configure AWS credentials](https://docs.aws.amazon.com/braket/latest/developerguide/braket-using-boto3.html) - Boto3 needs credentials to make API calls to AWS.\n",
    "- Install [Langchain](https://python.langchain.com/docs/get_started/introduction), a framework for developing applications powered by large language models (LLMs). Install with `pip install langchain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install boto3\n",
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 # to interact with AWS services.\n",
    "from langchain_community.document_loaders import PyPDFLoader # to load documents from PDF files.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # to split documents into smaller chunks.\n",
    "from langchain_community.vectorstores import FAISS # to store the documents in a vector database.\n",
    "from langchain_community.embeddings import BedrockEmbeddings # to create embeddings for the documents.\n",
    "from langchain_experimental.text_splitter import SemanticChunker # to split documents into smaller chunks.\n",
    "from langchain_core.documents import Document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client              = boto3.client(\"bedrock-runtime\") \n",
    "bedrock_embeddings          = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: PDF file to VectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_and_split_pdf` function load a PDF file, extract the text, and splits it into overlapping chunks based on character offsets using [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/).\n",
    "\n",
    "RecursiveCharacterTextSplitter splits a text into smaller chunks based on the maximum number of characters allowed per chunk. It works as follows:\n",
    "\n",
    "1. First, it tries to split the text into chunks using a separator, such as a whitespace or a line break `[\"\\n\\n\", \"\\n\", \" \", \"\"]`.\n",
    "\n",
    "2. If the resulting chunks exceed the maximum character limit, then it recursively splits each chunk into even smaller parts, until no chunk exceeds the limit.\n",
    "\n",
    "3. This recursive process continues until all chunks comply with the maximum character limit.\n",
    "\n",
    "The advantage of using RecursiveCharacterTextSplitter is that it divides the text in a more natural way, respecting sentence and paragraph boundaries whenever possible. This helps preserve the context and meaning of the original text, which is important for NLP tasks such as summarization, text generation, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Amazon_Bedrock_User_Guide.pdf\"\n",
    "path_file = \"demo-files\"\n",
    "file_path = f\"{path_file}/{file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_pdf(file_path, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter)\n",
    "    return docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "chunk_overlap = 100\n",
    "\n",
    "docs = load_and_split_pdf(file_path, chunk_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"documentos:\", len(docs))\n",
    "docs[6:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_and_split_pdf_semantic` function loads a PDF, splits the text into semantically meaningful chunks using [SemanticChunker](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker/), and returns the split documents. \n",
    "\n",
    "Unlike [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/) which divides the text based on a character limit, [SemanticChunker](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker/) uses a language model to understand the meaning and context of the text, and then divides it into sections that have a coherent meaning.\n",
    "\n",
    "The process works as follows:\n",
    "\n",
    "1. The full text and a language model are provided to the [SemanticChunker](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker/).\n",
    "\n",
    "2. The language model analyzes the text and divides it into semantically coherent sentences or segments. \n",
    "\n",
    "3. These segments are grouped into larger \"chunks\" using various techniques, such as topic analysis, topic change detection, etc.\n",
    "\n",
    "4. The resulting \"chunks\" represent sections of the text that have a coherent meaning and context.\n",
    "\n",
    "Additionally, you can use [Breakpoints](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker/#breakpoints) to have a more granular control over how the text is divided into chunks, which can be important to preserve the meaning and context of the original text during processing.\n",
    "\n",
    "The advantage of using SemanticChunker is that it produces text fragments that are easier to process and understand for subsequent language models, since each fragment has a coherent semantic meaning. This is especially useful for tasks such as summarization, information extraction, and answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_pdf_semantic(file_path, embeddings):\n",
    "    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_amount= 80)\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load_and_split(text_splitter)\n",
    "    print(f\"docs:{len(docs)}\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_docs = load_and_split_pdf_semantic(file_path, bedrock_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documentos: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Amazon Bedrock User Guide\\nPrompt engineering guidelines\\nTopics\\n•Introduction\\n•What is a prompt? •What is prompt engineering? •General guidelines for Amazon Bedrock LLM users\\n•Prompt templates and examples for Amazon Bedrock text models\\nIntroduction\\nWelcome to the prompt engineering guide for large language models (LLMs) on Amazon Bedrock. Amazon Bedrock is Amazon’s service for foundation models (FMs), which oﬀers access to a range of \\npowerful FMs for text and images. Prompt engineering  refers to the practice of optimizing textual input to LLMs to obtain desired \\nresponses.', metadata={'source': './demo-files/Amazon_Bedrock_User_Guide.pdf', 'page': 0}),\n",
       " Document(page_content='Prompting helps LLMs perform a wide variety of tasks, including classiﬁcation, question \\nanswering, code generation, creative writing, and more. The quality of prompts that you provide to \\nLLMs can impact the quality of their responses. These guidelines provide you with all the necessary \\ninformation to get started with prompt engineering. It also covers tools to help you ﬁnd the best \\npossible prompt format for your use case when using LLMs on Amazon Bedrock. Whether you’re a beginner in the world of generative AI and language models, or an expert with \\nprevious experience, these guidelines can help you optimize your prompts for Amazon Bedrock \\ntext models. Experienced users can skip to the General Guidelines for Amazon Bedrock LLM Users  or\\nPrompt Templates and Examples for Amazon Bedrock Text Models sections. Note\\nAll examples in this doc are obtained via API calls.', metadata={'source': './demo-files/Amazon_Bedrock_User_Guide.pdf', 'page': 0}),\n",
       " Document(page_content='The response may vary due to the \\nstochastic nature of the LLM generation process. If not otherwise speciﬁed, the prompts are \\nwritten by employees of AWS. Disclaimer: The examples in this document use the current text models available within Amazon \\nBedrock.', metadata={'source': './demo-files/Amazon_Bedrock_User_Guide.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"documentos:\", len(semantic_docs))\n",
    "semantic_docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any empty pages or documents without actual content.\n",
    "clean_docs = []\n",
    "for doc in docs:\n",
    "    if len(doc.page_content):\n",
    "        clean_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in clean_docs:\n",
    "    if len(doc.page_content) == 0:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Build Vector database](https://python.langchain.com/docs/integrations/vectorstores/faiss/#ingestion)\n",
    "Now, using Amazon Bedrock embeddings, create a vector database of document embeddings using [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/) that can allow quick searching by similarity and retrieval of related documents in the future. ,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Database:32 docs\n"
     ]
    }
   ],
   "source": [
    "db = FAISS.from_documents(clean_docs, bedrock_embeddings)\n",
    "print(f\"Vector Database:{db.index.ntotal} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Query](https://python.langchain.com/docs/integrations/vectorstores/faiss/#querying)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is a prompt?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Retriever](https://python.langchain.com/docs/integrations/vectorstores/faiss/#as-a-retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "docs = retriever.invoke(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Save to a local Vector database.](https://python.langchain.com/docs/integrations/vectorstores/faiss/#as-a-retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se guardo db en ./demo-files/Amazon_Bedrock_User_Guide.vdb\n"
     ]
    }
   ],
   "source": [
    "db_file_name = file_name.split(\".\")[0]\n",
    "db_file = f\"{db_file_name}.vdb\"\n",
    "db.save_local(db_file)\n",
    "print(f\"vectordb was saved in {db_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Query local Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file_name = \"Amazon_Bedrock_User_Guide.vdb\"\n",
    "new_db = FAISS.load_local(db_file_name, bedrock_embeddings, allow_dangerous_deserialization=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output that you generate using AI services \n",
      "is your content. Due to the nature of machine learning, output may not be unique across customers \n",
      "and the services may generate the same or similar results across customers. Additional prompt resources\n",
      "The following resources oﬀer additional guidelines on prompt engineering. •Anthropic Claude model prompt guide:  https://docs.anthropic.com/claude/docs\n",
      "•Anthropic Claude prompt engineering resources: https://docs.anthropic.com/claude/docs/\n",
      "guide-to-anthropics-prompt-engineering-resources\n",
      "•Cohere prompt guide:  https://txt.cohere.com/how-to-train-your-pet-llm-prompt-engineering\n",
      "•AI21 Labs Jurassic model prompt guide:  https://docs.ai21.com/docs/prompt-engineering\n",
      "•Meta Llama 2 prompt guide:   https://ai.meta.com/llama/get-started/#prompting\n",
      "•Stability documentation: https://platform.stability.ai/docs/getting-started\n",
      "•Mistral AI prompt guide:  https://docs.mistral.ai/guides/prompting-capabilities/\n",
      "What is a prompt? Prompts are a speciﬁc set of inputs provided by you, the user, that guide LLMs on Amazon Bedrock \n",
      "to generate an appropriate response or output for a given task or instruction. User Prompt:\n",
      "Who invented the airplane?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a prompt?\"\n",
    "docs = new_db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Delete Vectordb](https://python.langchain.com/docs/integrations/vectorstores/faiss/#delete)\n",
    "\n",
    "You can also delete records from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count before: 32\n",
      "count after: 31\n"
     ]
    }
   ],
   "source": [
    "print(\"count before:\", new_db.index.ntotal)\n",
    "new_db.delete([new_db.index_to_docstore_id[0]])\n",
    "print(\"count after:\", new_db.index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the entire database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method FAISS.delete of <langchain_community.vectorstores.faiss.FAISS object at 0x10feaadc0>>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_db.delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
