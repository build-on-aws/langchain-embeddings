{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Agentic video RAG with Strands Agents and Aurora PostgreSQL - Local infraestructure\n",
    "\n",
    "Build a comprehensive video content analysis system using [Amazon Bedrock](https://aws.amazon.com/bedrock/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el) with [Amazon Titan Multimodal Embeddings G1 model](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html), [Amazon Transcribe](https://aws.amazon.com/transcribe/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el) for speech-to-text conversion, and [Amazon Aurora PostgreSQL](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el) with pgvector extension for vector storage and similarity search.\n",
    "\n",
    "![Diagram](data/video-embedding.png)\n",
    "\n",
    "This notebook integrates with [Strands Agents](https://strandsagents.com/?trk=4f1e9f0e-7b21-4369-8925-61f67341d27c&sc_channel=el) to create intelligent agents with custom Python tools. You can build **two distinct agent types**:\n",
    "\n",
    "## ü§ñ Agent Architecture\n",
    "\n",
    "### 1. **Video Analysis Agent** \n",
    "> **Prerequisites**: ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è Create Amazon Aurora PostgreSQL with this [Amazon CDK Stack](https://github.com/build-on-aws/langchain-embeddings/tree/main/create-aurora-pgvector). Follow steps in [05_create_audio_video_embeddings.ipynb](/05_create_audio_video_embeddings.ipynb) ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "\n",
    "- **Purpose**: Processes and searches video content globally\n",
    "- **Capabilities**: Analyzes visual frames, transcribed audio, technical content\n",
    "- **Tools**: `video_embedding_local` for multimodal video search\n",
    "- **Use Case**: Technical content analysis, finding specific moments in videos\n",
    "\n",
    "![Diagram](data/agent_videoembedding_local.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Memory-Enhanced Agent**\n",
    "> **Prerequisites**: ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è Create Amazon Aurora PostgreSQL with this [Amazon CDK Stack](https://github.com/build-on-aws/langchain-embeddings/tree/main/create-aurora-pgvector). Follow steps in [05_create_audio_video_embeddings.ipynb](/05_create_audio_video_embeddings.ipynb) and create an [Amazon S3 verctor bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-vectors-buckets-create.html) that will serve as the backend for your vector memory. ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
    "\n",
    "- **Purpose**: Provides personalized, context-aware video analysis\n",
    "- **Capabilities**: Remembers user preferences, learns from interactions, provides tailored responses\n",
    "- **Tools**: `video_embedding_local` + `s3_vector_memory` for persistent user context\n",
    "- **Use Case**: Personalized learning experiences, adaptive content recommendations\n",
    "\n",
    "![Diagram](data/agent_videoembedding_local_memory.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "_region_name = \"us-east-1\"\n",
    "ssm = boto3.client(service_name=\"ssm\", region_name=_region_name)\n",
    "\n",
    "def get_ssm_parameter(name):\n",
    "    response = ssm.get_parameter(Name=name, WithDecryption=True)\n",
    "    return response[\"Parameter\"][\"Value\"]\n",
    "\n",
    "os.environ['AURORA_CLUSTER_ARN'] = get_ssm_parameter(\"/videopgvector/cluster_arn\")\n",
    "os.environ['AURORA_SECRET_ARN'] = get_ssm_parameter(\"/videopgvector/secret_arn\")\n",
    "os.environ['AURORA_DATABASE_NAME'] = 'kbdata'\n",
    "os.environ['AWS_S3_BUCKET'] = 'YOU-S3-BUCKET' \n",
    "os.environ['AWS_REGION'] = _region_name\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded from SSM at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ System Architecture\n",
    "\n",
    "This system provides comprehensive video analysis with these features:\n",
    "\n",
    "- **üé¨ Video Processing**: Extract frames using FFmpeg\n",
    "- **üß† Multimodal Embeddings**: Generate embeddings with Amazon Bedrock Titan\n",
    "- **üé§ Audio Transcription**: Convert audio to text with Amazon Transcribe\n",
    "- **üìä Vector Storage**: Store in Aurora PostgreSQL with pgvector\n",
    "- **üîç Semantic Search**: Perform similarity-based queries\n",
    "- **ü§ñ Agent Integration**: With Strands Agents framework\n",
    "\n",
    "The system extracts key frames, transcribes audio, generates embeddings for visual and text content, then stores everything in a searchable vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('tools')\n",
    "from video_embedding_tool_local import video_embedding_local\n",
    "\n",
    "print(\"‚úÖ Local video embedding tool imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Tool Configuration\n",
    "\n",
    "The `video_embedding_local` tool accepts these parameters:\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|----------|\n",
    "| `video_path` | Path to video (local or S3) | Required |\n",
    "| `user_id` | User identifier | Required |\n",
    "| `action` | 'process', 'search', 'list' | 'process' |\n",
    "| `similarity_threshold` | Similarity threshold (0.0-1.0) | 0.8 |\n",
    "| `frames_per_second` | Frame extraction rate | 1 |\n",
    "| `query` | Search query (for search action) | None |\n",
    "\n",
    "### Performance Optimization:\n",
    "- **High precision**: `frames_per_second: 2`, `similarity_threshold: 0.7`\n",
    "- **Balanced**: `frames_per_second: 1`, `similarity_threshold: 0.8`\n",
    "- **Fast processing**: `frames_per_second: 0.5`, `similarity_threshold: 0.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ID = \"langchain_test_user_2\"\n",
    "VIDEO_PATH = \"videos/video.mp4\"\n",
    "\n",
    "print(f\"üé¨ Processing video: {VIDEO_PATH}\")\n",
    "print(f\"üë§ User ID: {USER_ID}\")\n",
    "\n",
    "result = video_embedding_local(\n",
    "    video_path=VIDEO_PATH,\n",
    "    user_id=USER_ID,\n",
    "    action=\"process\",\n",
    "    similarity_threshold=0.8,\n",
    "    frames_per_second=1,\n",
    "    region=_region_name,\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Processing Result: {result.get('status')}\")\n",
    "if result.get('status') == 'success':\n",
    "    print(f\"Video S3 URI: {result.get('video_s3_uri')}\")\n",
    "    print(f\"Total frames: {result.get('total_frames')}\")\n",
    "    print(f\"Embeddings stored: {result.get('total_stored')}\")\n",
    "else:\n",
    "    print(f\"Error: {result.get('message')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Model Configuration Options\n",
    "\n",
    "Strands supports multiple model configuration approaches:\n",
    "\n",
    "### Option 1: Default Configuration\n",
    "```python\n",
    "from strands import Agent\n",
    "agent = Agent()  # Uses Claude 4 Sonnet by default\n",
    "```\n",
    "\n",
    "### Option 2: Specify Model ID\n",
    "```python\n",
    "agent = Agent(model=\"anthropic.claude-sonnet-4-20250514-v1:0\")\n",
    "```\n",
    "\n",
    "### Option 3: BedrockModel (Recommended)\n",
    "```python\n",
    "from strands.models import BedrockModel\n",
    "\n",
    "model = BedrockModel(\n",
    "    model_id=\"anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    temperature=0.3,\n",
    "    top_p=0.8\n",
    ")\n",
    "agent = Agent(model=model)\n",
    "```\n",
    "\n",
    "### Option 4: Anthropic Direct\n",
    "```python\n",
    "from strands.models.anthropic import AnthropicModel\n",
    "\n",
    "model = AnthropicModel(\n",
    "    model_id=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1028,\n",
    "    params={\"temperature\": 0.7}\n",
    ")\n",
    "```\n",
    "\n",
    "You can also use other model providers:\n",
    "- [LiteLLM](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/litellm/)\n",
    "- [llama.cpp](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/llamacpp/)\n",
    "- [Llama API](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/llamaapi/)\n",
    "- [Mistral AI](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/mistral/)\n",
    "- [Ollama](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/ollama/)\n",
    "- [OpenAI](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/openai/)\n",
    "- [Cohere](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/openai/)\n",
    "\n",
    "**BedrockModel Benefits:**\n",
    "- Native AWS integration\n",
    "- Guardrails support\n",
    "- Prompt caching capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "from s3_memory import s3_vector_memory\n",
    "from video_image_display import display_video_images\n",
    "\n",
    "# S3 Vectors Configuration\n",
    "os.environ['VECTOR_BUCKET_NAME'] = 'YOUR-S3-BUCKET'  # Your S3 Vector bucket\n",
    "os.environ['VECTOR_INDEX_NAME'] = 'YOUR-VECTOR-INDEX'        # Your vector index\n",
    "os.environ['AWS_REGION'] = 'us-east-1'                       # AWS region\n",
    "os.environ['EMBEDDING_MODEL'] = 'amazon.titan-embed-text-v2:0' # Bedrock embedding model\n",
    "\n",
    "model = BedrockModel(model_id=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\")\n",
    "\n",
    "VIDEO_SYSTEM_PROMPT = \"\"\"You are a video processing AI assistant.\n",
    "\n",
    "Available actions:\n",
    "- process: Upload and process videos \n",
    "- search: Search video content using semantic similarity\n",
    "- list: List all processed videos\n",
    "\n",
    "Use video_embeddings_aws for all cloud video operations.\n",
    "Use display_video_images to show search results.\n",
    "\"\"\"\n",
    "\n",
    "video_agent = Agent(\n",
    "    model=model, \n",
    "    tools=[video_embedding_local, display_video_images],\n",
    "    system_prompt=VIDEO_SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "memory_agent = Agent(\n",
    "    model=model, \n",
    "    tools=[video_embedding_local, s3_vector_memory, display_video_images],\n",
    "    system_prompt=VIDEO_SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Agents created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé• Test Video Content\n",
    "\n",
    "This notebook uses **AWS re:Invent 2024 session on \"AI self-service support with knowledge retrieval using PostgreSQL\"** ([YouTube link](https://www.youtube.com/watch?v=fpi3awGakyg)).\n",
    "\n",
    "**Video Topics:**\n",
    "- Vector databases and embeddings for AI applications\n",
    "- Amazon Aurora PostgreSQL with pgvector for scalable vector storage\n",
    "- RAG (Retrieval Augmented Generation) implementations\n",
    "- Amazon Bedrock Agents for intelligent customer support\n",
    "- Real-world use cases and technical demonstrations\n",
    "\n",
    "This content provides excellent testing material for our video analysis system with technical presentations including visual slides and detailed explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = video_agent(f\"\"\"What is the video about in {VIDEO_PATH}? \"\"\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† S3 Vector Memory Tool\n",
    "\n",
    "The `s3_vector_memory` tool provides AWS-native memory management using Amazon S3 with automatic user isolation. This tool complements video embedding functionality by storing user preferences and context.\n",
    "\n",
    "### Key Features:\n",
    "- **User Isolation**: Each user's memories are stored separately using `user_id`\n",
    "- **Persistent Storage**: Memories persist across agent sessions\n",
    "- **Semantic Search**: Find relevant memories using natural language queries\n",
    "- **AWS Native**: Uses Amazon S3 and Bedrock for embeddings\n",
    "\n",
    "### Available Actions:\n",
    "- `store`: Save new memory content for a user\n",
    "- `retrieve`: Search and retrieve relevant memories for a user\n",
    "- `list`: List all memories for a specific user\n",
    "\n",
    "### Tool Parameters:\n",
    "- `user_id`: **Required** - User identifier for memory isolation\n",
    "- `action`: Operation to perform (store/retrieve/list)\n",
    "- `content`: Content to store (for store action)\n",
    "- `query`: Search query (for retrieve action)\n",
    "- `top_k`: Maximum results to return (default: 20)\n",
    "\n",
    "This creates personalized agent experiences where the agent remembers user preferences, previous conversations, and context across multiple interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Tool Comparison\n",
    "\n",
    "Understanding how the two tools work differently:\n",
    "\n",
    "### `video_embedding_local` Tool:\n",
    "- **Search Scope**: Searches across ALL video content in the database\n",
    "- **User Context**: Uses `user_id` only for processing/listing operations\n",
    "- **Search Method**: Semantic search across video transcriptions and visual content\n",
    "- **Data Source**: Aurora PostgreSQL with pgvector\n",
    "\n",
    "### `s3_vector_memory` Tool:\n",
    "- **Search Scope**: Searches only within a specific user's memories\n",
    "- **User Context**: **Requires** `user_id` for all operations (security isolation)\n",
    "- **Search Method**: Semantic search across stored user memories\n",
    "- **Data Source**: Amazon S3\n",
    "\n",
    "This combination provides:\n",
    "1. **Global video search** across all content\n",
    "2. **Personal memory management** for individual users\n",
    "3. **Context-aware responses** that combine both data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = memory_agent(f\"\"\"I'm interested in learning about AI and database technologies. \n",
    "Store this preference for user {USER_ID}, then search the video in {VIDEO_PATH} content for technical discussions \n",
    "about vector databases, embeddings, and how they're used in AI applications.\"\"\")\n",
    "\n",
    "print(\"üß† Memory Agent - Store Preferences & Search:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = memory_agent(f\"\"\" What did the user  {USER_ID} ask before?\"\"\")\n",
    "\n",
    "print(\"üß† Memory Agent - Store Preferences & Search:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ID_1 = \"langchain_test_user_1\"\n",
    "\n",
    "response = memory_agent(f\"\"\" I'm interested in learning about RAG and SAAS. \n",
    "My user is {USER_ID_1}, explain the video in {VIDEO_PATH}.\n",
    "                        \"\"\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = memory_agent(f\"\"\" Mi nombre es Eli y quiero saber si el video en  {VIDEO_PATH} habla de whatsapp. \n",
    "My user is {USER_ID_1}.\n",
    "                        \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
