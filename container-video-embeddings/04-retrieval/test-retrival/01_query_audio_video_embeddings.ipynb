{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video and Audio Content Analysis with Amazon Bedrock and Amazon Aurora PostgreSQL pgvector\n",
    "\n",
    "This notebook demonstrates how to process video and audio content using [Amazon Bedrock](https://aws.amazon.com/bedrock/) to invoke [Amazon Titan Multimodal Embeddings G1 model](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) for generating multimodal embeddings, [Amazon Transcribe](https://aws.amazon.com/transcribe/) for converting speech to text, and [Amazon Aurora PostgreSQL](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html) with pgvector for efficient vector storage and similarity search, you will build an app that understands both visual and audio content, enabling natural language queries to find specific moments in videos.\n",
    "\n",
    "> Create Amazon Aurora PostgreSQL with this [Amazon CDK Stack](https://github.com/build-on-aws/langchain-embeddings/tree/main/create-aurora-pgvector)\n",
    "\n",
    "![Diagram](images/video-embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install boto3\n",
    "#!pip install json\n",
    "#!pip install base64\n",
    "#!pip install uuid\n",
    "# or install requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from PIL import Image as PILImage\n",
    "import random\n",
    "\n",
    "region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-west-2\")\n",
    "ssm = boto3.client(service_name=\"ssm\", region_name=region)\n",
    "sns_client = boto3.client('stepfunctions')\n",
    "\n",
    "\n",
    "# Default model settings\n",
    "default_model_id = os.environ.get(\"DEFAULT_MODEL_ID\", \"amazon.titan-embed-image-v1\")\n",
    "default_embedding_dimmesion = os.environ.get(\"DEFAULT_EMBEDDING_DIMENSION\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Interface (AuroraPostgres Class)\n",
    "\n",
    "An `AuroraPostgres` class that interacts with Amazon Aurora PostgreSQL [using RDS Data API](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html)\n",
    "\n",
    "Code: [aurora_service.py](create_audio_video_helper/aurora_service.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_audio_video_helper import AuroraPostgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Video Content Processing\n",
    "\n",
    "A `VideoProcessor` class uses the [ffmpeg libavcodec library](https://ffmpeg.org/) to proccess the audio and create frames. \n",
    "\n",
    "The class is set to process frames every 1 second, you can modify this by changing the FPS value in command.\n",
    "\n",
    "Code: [video_processor.py](create_audio_video_helper/video_processor.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embedding Generation\n",
    "\n",
    "Generate Embeddings for each extracted frame. Embeddins are created with the Amazon Titan Multimodal Embeddings G1 model using Amazon Bedrock Invoke Model API. \n",
    "\n",
    "Code: [embedding_generation.py](create_audio_video_helper/embedding_generation.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_audio_video_helper import EmbeddingGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The system uses environment variables and AWS Systems Manager Parameter Store for configuration:\n",
    "\n",
    "**DEFAULT_MODEL_ID:** Bedrock model ID (default: \"amazon.titan-embed-image-v1\")\n",
    "\n",
    "**DEFAULT_EMBEDDING_DIMENSION:** Embedding dimension (default: \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ssm_parameter(name):\n",
    "    response = ssm.get_parameter(Name=name, WithDecryption=True)\n",
    "    return response[\"Parameter\"][\"Value\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data from environment variables, never share secrets!\n",
    "\n",
    "cluster_arn = get_ssm_parameter(\"/videopgvector/cluster_arn\")\n",
    "credentials_arn = get_ssm_parameter(\"/videopgvector/secret_arn\")\n",
    "table_name = get_ssm_parameter(\"/videopgvector/video_table_name\")\n",
    "api_url = get_ssm_parameter(\"/videopgvector/api_retrieve\")\n",
    "bucket_name = get_ssm_parameter(\"/videopgvector/bucket_name\")\n",
    "state_machine_arn = get_ssm_parameter(\"/videopgvector/state_machine_arn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credentials_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Aurora PostgreSQL client\n",
    "aurora = AuroraPostgres(cluster_arn, table_name, credentials_arn,region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Aurora Cluster conectivity:\n",
    "aurora.execute_statement(\"select count(*) from bedrock_integration.knowledge_bases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_generation = EmbeddingGeneration(region,default_model_id,default_embedding_dimmesion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aurora.execute_statement(\"select count(*) from bedrock_integration.knowledge_bases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "base_path = \"images/\"\n",
    "\n",
    "def download_file(base_path,bucket, key, filename):\n",
    "    print(\"Download file from s3://{}{}\".format(bucket,key))\n",
    "    with open(base_path+filename, \"wb\") as data:\n",
    "        s3_client.download_fileobj(bucket, key, data)\n",
    "    print(\"Download file from s3://{}{}\".format(bucket,key))\n",
    "    return True\n",
    "\n",
    "def read_image_from_s3(s3_key):\n",
    "    parts = s3_key.split('s3://')[-1].split('/', 1)\n",
    "    bucket_name = parts[0]\n",
    "    image_key = parts[1]\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=image_key)\n",
    "        image_data = response['Body'].read()\n",
    "        return image_data\n",
    "    except Exception as e:\n",
    "        print(f'Error reading image from {s3_key}: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# Upload Video to Amazon S3 bucket\n",
    "def upload_file_to_s3 (video_path,bucket_name,s3_key):\n",
    "    s3_client.upload_file(video_path, bucket_name,s3_key)\n",
    "    print(\"Upload successful!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"<your-video-path>\"\n",
    "s3_key = f\"video/{video_path}\"\n",
    "upload_file_to_s3 (video_path,bucket_name,s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the status of the Step Functions workflow processing your video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sns_client.describe_state_machine(\n",
    "    stateMachineArn=state_machine_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sns_client.list_executions(\n",
    "    stateMachineArn=state_machine_arn,\n",
    "    maxResults=12\n",
    ")\n",
    "response['executions'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search\n",
    "\n",
    "Implements functions for:\n",
    "- `retrieve()`: Performs similarity searches in the database and displays results\n",
    "- `aurora.similarity_search()`: Executes the vector similarity search in the database\n",
    "- `get_embeddings()`: Generates embeddings for the search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def retrieve(search_query, how=\"cosine\", k=5):\n",
    "    search_vector = embedding_generation.get_embeddings(search_query)\n",
    "    \n",
    "    result = aurora.similarity_search(search_vector,how=how, k=k)\n",
    "    rows = json.loads(result.get(\"formattedRecords\"))\n",
    "    for row in rows:\n",
    "        metric = \"similarity\" if how == \"cosine\" else \"distance\"\n",
    "        metric_value = row.get(metric)\n",
    "        if row.get(\"content_type\") == \"text\":\n",
    "            print(\"row: \", row)\n",
    "            print (f\"text:\\n{row.get('chunks')}\\n{metric}:{metric_value}\\nmetadata:{row.get('metadata')}\\n\")\n",
    "        if row.get(\"content_type\") == \"image\":\n",
    "            print(row)\n",
    "            sourceurl = row.get('sourceurl')\n",
    "            print(sourceurl)\n",
    "            bucket_name = sourceurl.split(\"/\")[2] \n",
    "            key = sourceurl.replace(\"s3://\", \"\").replace(bucket_name,\"\").lstrip(\"/\")\n",
    "            filename = sourceurl.split(\"/\")[-1] \n",
    "            print(\"bucket_name: \",bucket_name)\n",
    "            print(\"key: \",key)\n",
    "            print(\"filename: \",filename)\n",
    "            download_file(base_path,bucket_name, key, filename)\n",
    "            img = PILImage.open(base_path+filename)            \n",
    "            print (f\"Image:\\n{row.get('sourceurl')}\\n{metric}:{metric_value}\\nmetadata:{row.get('metadata')}\\n\")\n",
    "            display(img)\n",
    "        del row[\"embedding\"]\n",
    "        del row[\"id\"]\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested the notebook with my AWS re:Invent 2024 sesion [AI self-service support with knowledge retrieval using PostgreSQL](https://www.youtube.com/watch?v=fpi3awGakyg?trk=fccf147c-636d-45bf-bf0a-7ab087d5691a&sc_channel=video). \n",
    "\n",
    "I ask for Aurora and it brings me images and texts where it mentions:\n",
    "\n",
    "![Diagram](data/cosine.png)\n",
    "\n",
    "```bash\n",
    "text:\n",
    "memory . A place where all the information is stored and can easily be retrievable , and that's where the vector database comes in . This is the the first building block . And a vector database stores and retrieves data in the form of vector embeddeds or mathematical representations . This allows us to find similarities between data rather than relying on the exact keyword match that is what usually happens up to today . This is essential for systems like retrieval ofmented generation or RAC , which combines external knowledge with the AI response to deliver those accurate and context aware response . And by the way , I think yesterday we announced the re-rank API for RAC . So now your rack applications , you can score and it will prioritize those documents that have the most accurate information . So at the end will be even faster and cheaper building rack . We're gonna use Amazon Aurora postgrade SQL with vector support that will give us a scalable and fully managed solution for our AI tasks .\n",
    "similarity:0.5754164493071239\n",
    "metadata:{\"speaker\":\"spk_0\",\"second\":321}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"aurora\"\n",
    "docs = retrieve(search_query, how=\"cosine\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vector = embedding_generation.get_embeddings(search_query)\n",
    "result = aurora.similarity_search(search_vector,how=\"cosine\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = json.loads(result.get(\"formattedRecords\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"elizabeth\"\n",
    "docs = retrieve(search_query, how=\"l2\", k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Implementation\n",
    "\n",
    "Finally, the notebook implements a complete RAG system:\n",
    "- `CustomMultimodalRetriever`: A custom retriever class that extends BaseRetriever\n",
    "- `_get_relevant_documents()`: Core retrieval method that finds similar content\n",
    "- `image_content_block()`: Formats image content for LLM consumption\n",
    "- `text_content_block()`: Formats text content for LLM consumption\n",
    "- `parse_docs_for_context()`: Processes documents for context (text and images)\n",
    "- `ThinkingLLM`: Uses an LLM to answer questions based on retrieved content\n",
    "\n",
    "> Based on https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_retriever.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "class CustomMultimodalRetriever(BaseRetriever):\n",
    "    \"\"\"A retriever that contains the top k documents that contain the user query.\n",
    "    query could be text or image_bytes\n",
    "    \"\"\"\n",
    "    k: int\n",
    "    \"\"\"Number of top results to return\"\"\"\n",
    "    how: str\n",
    "    \"\"\"How to calculate the similarity between the query and the documents.\"\"\"\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "        search_vector = embedding_generation.get_embeddings(query)\n",
    "        result = aurora.similarity_search(search_vector, how=self.how, k=self.k)\n",
    "        rows = json.loads(result.get(\"formattedRecords\"))\n",
    "\n",
    "        matching_documents = []\n",
    "\n",
    "        for row in rows:\n",
    "            document_kwargs = dict(\n",
    "                metadata=dict(**json.loads(row.get(\"metadata\")), content_type = row.get(\"content_type\"), source=row.get(\"sourceurl\")))\n",
    "            \n",
    "            if self.how == \"cosine\":\n",
    "                document_kwargs[\"similarity\"] = row.get(\"similarity\")\n",
    "            elif self.how == \"l2\":\n",
    "                document_kwargs[\"distance\"] = row.get(\"distance\")\n",
    "\n",
    "            if row.get(\"content_type\") == \"text\":\n",
    "                matching_documents.append( Document( page_content=row.get(\"chunks\"), **document_kwargs ))\n",
    "            if row.get(\"content_type\") == \"image\":\n",
    "                matching_documents.append( Document( page_content=row.get(\"source\"),**document_kwargs ))\n",
    "\n",
    "        return matching_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = CustomMultimodalRetriever(how=\"cosine\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"como funciona aurora\"\n",
    "docs = retriever.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=_region_name)\n",
    "\n",
    "\n",
    "budget_tokens = 0\n",
    "max_tokens = 1024\n",
    "conversation: List[Dict] = []\n",
    "reasoning_config = {\"thinking\": {\"type\": \"enabled\", \"budget_tokens\": budget_tokens}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_content_block(sourceurl):\n",
    "    print(\"sourceurl: \",sourceurl)\n",
    "    bucket_name = sourceurl.split(\"/\")[2] \n",
    "    key = sourceurl.replace(\"s3://\", \"\").replace(bucket_name,\"\").lstrip(\"/\")\n",
    "    filename = sourceurl.split(\"/\")[-1] \n",
    "    print(\"bucket_name: \",bucket_name)\n",
    "    print(\"key: \",key)\n",
    "    print(\"filename: \",filename)\n",
    "    image_bytes = read_image_from_s3(sourceurl)\n",
    "    extension = filename.split('.')[-1]\n",
    "    print (f\"Including Image :{filename}\")\n",
    "    if extension == 'jpg':\n",
    "        extension = 'jpeg'\n",
    "    \n",
    "    block = { \"image\": { \"format\": extension, \"source\": { \"bytes\": image_bytes}}}\n",
    "    print(block)\n",
    "    return block\n",
    "\n",
    "def text_content_block(text):\n",
    "    return { \"text\": text }\n",
    "\n",
    "def parse_docs_for_context(docs):\n",
    "    blocks = []\n",
    "    for doc in docs:\n",
    "        if doc.metadata.get('content_type') == \"image\":\n",
    "            print(doc)\n",
    "            blocks.append(image_content_block(doc.metadata.get('source')))\n",
    "        else:\n",
    "            print(doc)\n",
    "            blocks.append(text_content_block(doc.page_content))\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(model_id,system_prompt,content) -> str:\n",
    "    \"\"\"Get completion from Claude model based on conversation history.\n",
    "\n",
    "    Returns:\n",
    "        str: Model completion text\n",
    "    \"\"\"\n",
    "\n",
    "    # Invoke model\n",
    "    kwargs = dict(\n",
    "        modelId=model_id,\n",
    "        inferenceConfig=dict(maxTokens=max_tokens),\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            }\n",
    "        ],\n",
    "\n",
    "    )\n",
    "\n",
    "    kwargs[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    response = bedrock_runtime.converse(**kwargs)\n",
    "    \n",
    "    return response.get(\"output\",{}).get(\"message\",{}).get(\"content\", [])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"\"\"Answer the user's questions based on the below context. If the context has an image, indicate that it can be reviewed for further feedback.\n",
    "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\". (IF YOU MAKE SOMETHING UP BY YOUR OWN YOU WILL BE FIRED). For each statement in your response provide a [n] where n is the document number that provides the response. \"\"\"\n",
    "model_id = \"us.amazon.nova-pro-v1:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"<your-query>\"\n",
    "docs = retriever.invoke(query)\n",
    "parsed_docs = parse_docs_for_context(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response = answer(model_id,system_prompt,[text_content_block(f\"question:{query}\\n\\nDocs:\\n\"), *parsed_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_response[0].get(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"<your-query>\"\n",
    "docs = retriever.invoke(query)\n",
    "parsed_docs = parse_docs_for_context(docs)\n",
    "llm_response = answer(model_id,system_prompt,[text_content_block(f\"question:{query}\\n\\nDocs:\\n\"), *parsed_docs])\n",
    "print(llm_response[0].get(\"text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query thought Amazon Lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicializar el cliente de Lambda\n",
    "lambda_client = boto3.client('lambda', region_name=\"us-east-1\")\n",
    "\n",
    "# 1. Invocación Síncrona (RequestResponse)\n",
    "def invoke_lambda_sync(function_name, payload):\n",
    "    try:\n",
    "        response = lambda_client.invoke(\n",
    "        FunctionName=function_name, \n",
    "        InvocationType='RequestResponse',\n",
    "        Payload=json.dumps(payload)\n",
    "        )\n",
    "        response_payload = json.loads(response['Payload'].read())\n",
    "        return response_payload\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Lambda: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_name = \"<your-lambda>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"id\": \"<your-video-name>\",\n",
    "    \"query\": \"<your-query>\",\n",
    "    \"method\":\"retrieve_generate\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_payload = invoke_lambda_sync(function_name, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
